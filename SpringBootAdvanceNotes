
https://github.com/eazybytes/microservices
PDF need for revision
quesiton:
ENtity vs DTO.
DTO-https://martinfowler.com/eaaCatalog/dataTransferObject.html
Auditing and transacitons in spring data jpa:-
https://medium.com/@lavishj77/spring-data-jpa-auditing-and-transaction-management-b9e3246c2968
Documentation of Rest api:-we need add dependency of springdoc-openapi and it will add the swagger 
document of our apis through swagger endpoint and we can add more details using annotation from this dependencies.
                            -:Strangler pattern for microservice migration:-
https://www.geeksforgeeks.org/system-design/strangler-pattern-in-micro-services-system-design/

                                                          ----Docker ------
when we are using cloud services, cloud service providers have their physical infar at their data centers called servers but those are huge infra so, they can
not be given to single org .so, with help of Hypervisior concept they divide the servers resouces like ram, memory etc viutually into different parts called virtual
Matchine(VM), these vms are work as computer having memomory,os etc, so, we can use these vms according to our need of our application .
->Suppose we want to run our one of the microservices to the vm so, we need to install java version needed, data base and other dependencies so,we can run our serivce but
there is one issue with it,its not sure that our service will need all the resouces of the vm , in that case it will do the over price and for n number of services
we need to have same number of vm which is not a good practice. neither we can deploye all our services to same vm , that might cause resouces issue, dependencies issue
like java version,diff db etc.similerly scaling will be required manual intervention so, this way vm are not right for our microservices.

  VM
vm(os,libranry etc)
Hypervisior
Servers

solution of this problem is Container->its name suggest it contains all the required dependencies for particular service(simile ship container contains all needed resouces
in it for like aaple container will have ac in it)

COntainers
container1(lib,dependecies)
container engine(Docker)
Host OS
Servers

=>Even os is not there in container so,its light weight  and its super quick to restart  and destroy.
->WE will create image of our container which will be representation of our container, we can create any no of container from image like any no of obj we can create from
java class.
->Docker container is running repsentation of running image.

Follow notes from -file:///C:/Users/abhijeetkumar5/Desktop/LearningNotes-main/Learning/java%20interview%20questions/Advance%20spring%20boot/Master+Microservices+with+SpringBoot,Docker,Kubernetes.pdf


                                                                      Redis
download and unzip and start server, it will ready for accpeting traffic.
:-https://github.com/microsoftarchive/redis/releases/tag/win-3.2.100
Redis (which stands for REmote DIctionary Server) is an open-source, in-memory data structure store that is used as a database, cache, and message broker. 
Because all data resides in main memory (RAM), it delivers extremely high performance with sub-millisecond latency. 
=>https://github.com/Java-Techie-jt/spring-data-redis/tree/main =>Redis as in memory db
                                                      Redis for caching
https://github.com/Java-Techie-jt/spring-data-redis-cache
when  @GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product") added at the method of serive or controller layer, first search take the data from db but next it took it from cache,
we find diff in time of response in the postman along with log of calling db is not there when we hit sencond timee.
 @Cacheable(key = "#id",value = "product") //Id is the key of cache and value is the hash of cachedb.

=>@GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product",unless = "#result.price>1000") //here product will be cache based on condition if price is less than 1000 only.
=>When we are deleting the data from db, we will have remove that data from cache too, here we do so:-

@DeleteMapping("/{id}")
    @CacheEvict(key="#id",value = "product")// key and value would same as we using in cacheable. it will remove the data from cache.
 @CachePut(value = "users", key = "#user.id")=> THis will update the the data to cache when we add new data to user.

-----------------------------------------------------------------------Creating image of our application using docker-------------------------------------
First we have to mention the packing of our application in pom.xml file at place where name and verion of applicaiton is mentioned.
===>> <packaging>jar</packaging>
=>Before we make code changes for docker, when we build the application, we will getting the jar craeted for our app, which will have all code and lib but not java runtime
this is also called fat jar having big sizw e.g- accounts-0.0.1-SNAPSHOT.jar
This is created with ref of name from pom:-
<artifactId>accounts</artifactId>
<version>0.0.1-SNAPSHOT</version>
=>mvn spring-boot:run ---->Runnnig from cmd.
=>java -jar target/accounts-0.0.1-SNAPSHOT.jar ------------------->run spring boot using java
--------------------------------------------------------Creating Docker file --------------------
#Start with a base image containing Java runtime:tag like version of jdk image.Java is need for our application to run so, we are using java image which is there at dockerhub.
FROM openjdk:21-jdk-slim

# MAINTAINER instruction is deprecated in favor of using label
# MAINTAINER eazybytes.com
#Information around who maintains the image
LABEL "org.opencontainers.image.authors"="eazybytes.com"

# Add the application's jar to the image
//here we are copying our jar file from target folder which is generated post build to image(which has java and now our applicaiton is there with)

COPY target/accounts-0.0.1-SNAPSHOT.jar accounts-0.0.1-SNAPSHOT.jar

# execute the application, this is the same cmd which we used to run our app using java cmd , there was space in bwn but here commona
ENTRYPOINT ["java", "-jar", "accounts-0.0.1-SNAPSHOT.jar"]


--------------------------------------------------------------------Running docke image-------------------------------------------
First check if docker is running in our system, cmd:- docker version
=>Build docker image: ->docker build . -t easybyte/accounts:s4
format:-docker build path -t(tag) userId/nameOfapp:tag(version)
->When we build it from cmd, first it will download the openjdk from dockerhub and try to make the image by copying our jar file and jdk to it.
------------------------------------------------Running docker containere---------------
docker run -p 8080:8080 ak608333319/accounts:s4 :--p(port fisrt port is for our local system, second port is for docker to expose at 8080 , since docker run on 
isolated network, we can run same image on same port multiple at same time since, they are isolated so, every private network can run on same)
docker run -d -p 8080:8080 ak608333319/accounts:s4 //-d means runnig in detach mode so, other cmd can be fired form cmd while one is running
docker run -d -p 8081:8080 ak608333319/accounts:s4

Above two containers are runnig same image but on diff port  , so we have scaled the our app to two pod here.
We are using same port number for docker to expose for two diff pod is mained by docker call Port mapping .

`````````````````````````````````````````````````````````Buildpacks--------------
Buildpacks are a technology that automates the process of turning application source code into runnable container images, typically without the need for a Dockerfile.
They achieve this by automatically detecting the programming language and framework of an application, installing necessary dependencies,
and configuring the runtime environment.

we will add jar packing to the pom file:-
<packaging>jar</packaging>

we will add the image tag to the plugin section of the pom file:-

                 <configuration>
                    <image>
                        <name>eazybytes/${project.artifactId}:s4</name>
                    </image>
                </configuration>

=>Run the build cmd which will scan the whole project and build the image using all dependencies and we do not have to write the insturction to include
java veresion copy the  fat jar from tartet to our image etc.
=>This is required to run the docker in background.
=>mvn spring-boot:build-image  //Generating docker image form mvn.
==>RUnning the new image generated:-
docker images =>get the list of image
=>docker run -d -p 8090:9090 eazybytes/loans:s4 =>run the image with its name.
we will able to see image is running  on docker , we can hit the api from postman to validate too.

-------------------------------------------------------------------------Pushing docker image to DockerHub-----------

docker images
docker image push docker.io/ak608333319/accounts:s4 

//this will push the image like same we does in github, here we have already login from our account in docker desktop
so, cread will be taken from there and our image will be pushed to hub without explicity asked for creds.

we can see this in reps:-https://hub.docker.com/repository/docker/ak608333319/accounts/general //same image can be used in pipeline to deployed in cloud.
=>First delete the accounts container and image then we will try to pull the image from hub and run it.

Anyone want to use this they can pull it:-docker pull ak608333319/accounts:s4
=>ocker run -d -p 8080:8080 ak608333319/accounts:s4 //running the image pulled from dockerhub
                                                    ----docker compose--------
Docker Compose is a tool for defining and running multi-container Docker applications. It simplifies the orchestration of multiple services that work together to form a
complete application, such as a web server, a database, and a caching layer. 
Docker Compose uses a single YAML file (typically compose.yml or docker-compose.yml) to define all the services, networks, and volumes required for your application.
This centralizes the configuration and makes it easy to manage.
sample file:-we will maintain the herarchie of yml

services:                                //this is the root level where list of services will be mentioned 
  accounts:                              //Microservice level infos
    image: "eazybytes/accounts:s4"       //image of account serice
    container_name: accounts-ms          //name of container else random will be assigned as we saw in desktop(docker)
    ports:                               //port at which docker and our sevices will listen
      - "8080:8080"
    deploy:                              //here we gave limitation to our app which uses the resources of docker
      resources:
        limits:
          memory: 700m                   //max 700 mb will be used by account services
    networks:                            //as we know docker run in isolated network so, if our services need to communicated with each other we need to have common network
      - eazybank
  loans:
    image: "eazybytes/loans:s4"
    container_name: loans-ms
    ports:
      - "8090:8090"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
  cards:
    image: "eazybytes/cards:s4"
    container_name: cards-ms
    ports:
      - "9000:9000"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
networks:                    //here we are defining the common network where all our apps will be running, network name is easybank and bridge will be used as driver to communicate
  eazybank:
    driver: "bridge"


We can run the compose file with cmd so, all of the services image will start running together, we do not have to do it each and every service wise neither we
have to create docker file for each service.

CMD:->docker compose up -d {running in detach mode, it will download the all dependencies and start our image , we need to hit the cmd from the location where our compose file}
      //created containers
CMD->docker copose down -d {stopped and revmove the running image}, it will be removed we can check by doing docker ps.
CMD->docker compose start (start the stopped containers only, not removed containers)
CMD->docker compose stop (stop the the running container, if not running container it will give error message)


                                            ----Imp cmd of Docker--------
Docker Commands used in the course                       Docker Command	Description

"docker build . -t eazybytes/accounts:s4"   	            To generate a docker image based on a Dockerfile
"docker run -p 8080:8080 eazybytes/accounts:s4"	          To start a docker container based on a given image
"docker images"                                          	To list all the docker images present in the Docker server
"docker image inspect image-id"	                          To display detailed image information for a given image id
"docker image rm image-id"	                              To remove one or more images for a given image ids
"docker image push docker.io/eazybytes/accounts:s4"	      To push an image or a repository to a registry
"docker image pull docker.io/eazybytes/accounts:s4"	      To pull an image or a repository from a registry
"docker ps"		                                            To show all running containers
"docker ps -a"		                                        To show all containers including running and stopped
"docker container start container-id"		                  To start one or more stopped containers
"docker container pause container-id"	                  	To pause all processes within one or more containers
"docker container unpause container-id"		                To unpause all processes within one or more containers
"docker container stop container-id"		                  To stop one or more running containers
"docker container kill container-id"		                  To kill one or more running containers instantly
"docker container restart container-id"	                  To restart one or more containers
"docker container inspect container-id"		                  To inspect all the details for a given container id
"docker container logs container-id"		                  To fetch the logs of a given container id
"docker container logs -f container-id"		                  To follow log output of a given container id
"docker container rm container-id"		                  To remove one or more containers based on container ids
"docker container prune"		                              To remove all stopped containers
"docker compose up"		                 	                   Creates and starts containers based on the given Docker Compose file
"docker compose down"		                 	                   Stops and removes containers, networks, volumes, and images created by up
"docker compose start"		                                Starts existing (previously created) containers without recreating them
"docker compose stop"		                                  Stops running containers without removing them
"docker run -p 3306:3306 --name accountsdb -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=accountsdb -d mysql"        	        To create a MySQL DB container
"docker run -p 6379:6379 --name eazyredis -d redis"        	To create a Redis Container
"docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:22.0.3 start-dev"	To create Keycloak Container



                                                                            DOcker extention
Docker Extensions are add-ons that integrate third-party tools and functionalities directly into Docker Desktop, enhancing its capabilities. 
They allow users to seamlessly connect their preferred development tools to their application development and deployment workflows. 
 section 4 code:-https://github.com/eazybytes/microservices/tree/3.4.1/section4

Cloud native application:-
A cloud-native application is a software application designed and built specifically to run in a cloud environment, taking full advantage of its scalability,
flexibility, and resilience. These applications are built using a microservices architecture, where functionality is broken down into small, independent services.
Key technologies include containerization (like Docker and Kubernetes), CI/CD pipelines, and serverless functions. 
cloud native app can be on gcp,aws,azure etc.
Key characteristics

Agile development: They support agile development practices, which allows for rapid deployment of new features and faster time-to-market. 
Scalability and resilience
Automation:ci/cd
Containerization:docker or othe tool
Microservices architecture:

                                                =======12 factos and 15 factor Methodology============
The "15-Factor Methodology" is a set of principles for developing modern, cloud-native applications, an extension of the original "Twelve-Factor App" methodology. 
The 15 factors are:
1.One Codebase, one Application : Each application has only one code base which will be independently deployeing using packing (build once) and run in multiple env
we should have config of diff env externally injected during deployement, so, once its build will be running same in all env.Its easy to maintain and tack.

2.API First: Applications should be designed with an API-first approach, defining service contracts upfront using a language like Open API Specification (Swagger).
dictates that applications should be designed with their Application Programming Interfaces (APIs) as "first-class citizens" from the outset of the development process.
This means the API contract and design are defined and agreed upon before writing any application code or building user interfaces. 

3.Dependency Management: Explicitly declare and isolate dependencies using a dependency declaration manifest and an isolation tool.we use maven or gradle where we declr
our dependencies which will be download and mangaed by it, we don't need to download explicitely.

4.Design, Build, Release, Run: Strictly separate the build, release, and run stages to create an immutable build artifact at each release.All setsps shold be followed and 
does not  make any changes at any stage , so make it immutable. 

5.Config: Store configuration that varies between environments (staging, production, etc.) in environment variables or external configuration services, never in the code.

6.Logs: Treat logs as event streams, directing them to standard output and leaving the aggregation and storage to external tools.so, we can see the logs of all the MS at single place.

7.Disposability: Maximize robustness by designing processes that can be started or stopped at a moment's notice with fast startup and graceful shutdown.In the cloud native
we have docker which make our application to light image which will be easy to deploye and stopped if needed , while stopping our aaplicatin should not take more req and 
complete the ongoing req, in case of neeed it will upscale and downsacle autometically, if somthing went wrong it will be restart the app, these capablity is acheive by
kubernative, so,

8.Backing Services: Treat all backing services (databases, message brokers, caching systems, etc.), whether local or third-party, as attached resources accessed via a URL/credentials stored in the config.

9.Environment Parity: Keep development, staging, and production environments as similar as possible to minimize gaps in time, people, and tools.

10.Administrative Processes: Run admin/management tasks (database migrations, batch jobs, etc.) as one-off processes in an identical environment to the regular long-running app processes.
Thease should be treated as isolated process and deplouyed in each env.we should deployed thease as seperate MS.

11.Port Binding: Applications should be self-contained and expose their services via port binding, not relying on runtime injection of a web server.

12.Stateless Processes: Execute the app as one or more stateless and share-nothing processes, storing any necessary persistent data in a stateful backing service.
should not store any data in session or any where expect db,it won't have same instance in next req, so, all instance are isolated to each other.

13.Concurrency: Scale out via the process model, designing applications to distribute workload across multiple processes.
In java app JVM handle this multi threading by using thread from thread pool, by default we have 200 thread avail to serve the client.

14.Telemetry: Design the application to include the collection of monitoring data, health information, and statistics (APM, domain-specific data, system logs).

25.Authentication & Authorization: Implement robust security measures, especially for APIs, using solutions like OAuth2 or OpenID Connect to handle user authentication and authorization. 

                                      ==========Configuration Management ============
If we bind all code and config to the application, we need to build the image in every other env since, our config is hard binded to code, so this process would be not be
good practice, so, config should be externally managed and once our code image is build can be used in all env , out config would be inject at the time of deployment
it would be env wise so, we will be free from overhead of config.

=>Spring boot let us externalise the configuration so, we can use the same code base in all env .
=>Bydefault Spring boot look for config in application.properties, it has set of priority which will override the configs;-
application .properties<os Env veriable<java system properties(System.getProperties())<JIndi components<Servelet init paramenters<servelet config parameters<command line arg.

=>we can read the config injava using 
1.@value , it will read one value at time
@Value("${build.version}")
    private String buildVersion;

2.Environement interface->env.getProperties() one config at time, this is used to read the env veriable like secret,password;
@Autowired
private Environment environment;
 @GetMapping("/java-version")
    public ResponseEntity<String> getJavaVersion() {
        return ResponseEntity
                .status(HttpStatus.OK)
                .body(environment.getProperty("JAVA_HOME"));//this will return the path of our java insalled in system.
    }

3.@ConfigurationProperties("prefix")->we can define the pojo and it will read the all config with specified prefix to a pojo.

confog from yml:-
accounts:
  message: "Welcome to EazyBank accounts related local APIs "
  contactDetails:
    name: "John Doe - Developer"
    email: "john@eazybank.com"
  onCallSupport:
    - (555) 555-1234
    - (555) 523-1345

Reading into pojo of Record class:-
@ConfigurationProperties(prefix = "accounts")
public record AccountsContactInfoDto(String message, Map<String, String> contactDetails, List<String> onCallSupport) {
//THe dto will be populated during starting of app.
}
@EnableConfigurationProperties(value = {AccountsContactInfoDto.class}) we need to add this to our application class.

The @EnableConfigurationProperties annotation can be declared in two primary places:
On the main application class (the class annotated with @SpringBootApplication).
On a separate @Configuration class. 
How Does It Work?
@ConfigurationProperties: This annotation defines a class where external configuration properties will be mapped. You can specify a prefix that matches the keys in your application.properties or application.yml file.
@EnableConfigurationProperties: This annotation is applied to a Spring configuration class and specifies which configuration properties classes should be bound from external configuration sources.
                                                   
E.g-
@ConfigurationProperties(prefix = "app")
public class AppProperties {
    private String name;
    private String version;
}

@Configuration
@EnableConfigurationProperties({AppProperties.class, DatabaseProperties.class})
public class AppConfig {
}

                                                                                  ------Profiles-----
=>Spring provides a great tool for grouping configuration properties into so-called profiles(dev, qa, prod) allowing us to activate a bunch of configurations based on the active
profile.
=>The default profile is always active. Spring Boot loads all properties in application.properties into the default profile.We can create another profiles by creating property files like below,
=>application_prod.properties ---------> for prod profile
=>application_qa.properties ---------> for QA profile
=>We can activate a specific profile using spring.profiles.active property like below,
spring.profiles.active=prod
=>When we have common properties which are used by all env , we will keep in application.properties/yml file and env specific config would be there inside the env files.
e.g for prod:-

spring:
  config:
    activate:
      on-profile: "prod"  //similer in each yml file for marking it for that env

TO Activate that env we will use this config in application.yml file:-
Spring:
  config:
    import:
      - "application_qa.yml"  //we will import all config files to default config
      - "application_prod.yml"
  profiles:
    active:    //activating the specific env profile.
      - "qa"
=>We can externalize the configuration so, we can update the profile externally, we can do it by USing Command Line Argument from Inetlij, program arg from run config:-
--spring.profile.active="qa" --build.version="1.1"   //we will use -- to overide the properties file values
=>With Help of VM argument:-Dbuild.version="1.2" -Dspring.profile.active="qa"  //-D prefix is added to the profperties
=>ENV veriable form config:BUILD_VERION=1.3;SPRING_PROFILE_ACTIVE=qa  //all key would be in capital and . will be replace with _
priority->cmd-arg>jvm arg> file config

                              -----Drawbacks of externalized configurations using SpringBoot alone -----
challenges of using standard methods like CLI arguments, JVM properties, and environment variables for managing application configuration,
and poses six specific questions regarding advanced configuration management strategies.
=>THis req manual intervention, service restart,Admin can see the password or cred used in these args, so, this is not full proof solution for externalization of config.

                                          =============Spring Cloud Config =============
Spring Cloud Config provides server and client-side support for externalized configuration in a distributed system. With the Config Server you have a central place to 
manage external properties for applications across all environments. The concepts on both client and server map identically to the Spring Environment and PropertySource 
abstractions, so they fit very well with Spring applications, but can be used with any application running in any language. As an application moves through the deployment
pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run
when they migrate. The default implementation of the server storage backend uses git so it easily supports labelled versions of configuration environments, as well as being 
accessible to a wide range of tooling for managing the content. It is easy to add alternative implementations and plug them in with Spring configuration.

Features
Spring Cloud Config Server features:

HTTP, resource-based API for external configuration (name-value pairs, or equivalent YAML content)
Encrypt and decrypt property values (symmetric or asymmetric)
Embeddable easily in a Spring Boot application using @EnableConfigServer
Config Client features (for Spring applications):

Bind to the Config Server and initialize Spring Environment with remote property sources
Encrypt and decrypt property values (symmetric or asymmetric)

@EnableConfigServer->Mark the service as Config server

=>will have cloud config dependencies. we will put all the configs of loan,account,card services to the resources of this service which will be used to provide to its client
services which they are.Each env and service config would be there in config folder of the server service.

                                                                      ----application.yml file of cloud config server MS:-

spring:
  application:
    name: "configserver"   //name of service
  profiles:
    active: native         //we use native in case of file is stored in classpath
  cloud:
    config:
      server:
        native:
          search-locations: "classpath:/config"  //location where all configs are stored
server:
  port: 8071


===>When we try to run the application, we can get the all the configuration using sample endpoint:-http://localhost:8071/accounts/prod
accounts-prod.yml->file name present in config folder of classpath.
=>With this name:-accounts.yml->http://localhost:8071/accounts/default , this will return default properties.
=>Now our config server is rady to serve the config through endpoint.
=>update the server file location to:-search-locations: "file:///Users//eazybytes//Documents//config"

                          ---------------------Client application for Accounts service--------------
=>yml file for client service :-
server:
  port: 8080
spring:
  application:
    name: "accounts"   //name of the service will be used to req the config from the server.
  profiles:
    active: "prod"      //this will be used to load the profile of the config-name passed from clint to server
  config:
    import: "optional:configserver:http://localhost:8071/"  //this wil load the config server which is running at port 8071, according to name and profile given here 
														//properties will be loaded to our ms, optional is something which will be ignore and run our applicatin in case server conntion failed .


we need to add config dependencies:-
<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-config</artifactId>
		</dependency>

version:-<spring-cloud.version>2024.0.0</spring-cloud.version>
sme need to add in dependencies management:-
<dependencyManagement>
		<dependencies>
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-dependencies</artifactId>
				<version>${spring-cloud.version}</version>
				<type>pom</type>
				<scope>import</scope>
			</dependency>
		</dependencies>
	</dependencyManagement>

											------Reading configs from file system in config-server----------
==>when we will run accounts services, we will get the data from config server for prod:-http://localhost:8080/api/contact-info
->Here we are readiing the config from .properties file using 
@EnableConfigurationProperties(value = {AccountsContactInfoDto.class})   //Main class level
@ConfigurationProperties(prefix = "accounts")  //DTO class level.
											---Reading config from Git---

spring:
  application:
    name: "configserver"
  profiles:
     active: git    //to read the config from git
  cloud:
    config:
      server:
        git:  
          uri: "https://github.com/eazybytes/eazybytes-config.git"  //repo where configs are placed
														=>We can read it from private git or other place, we can refer the offical doc for other way to read the configs.
          default-label: main   //branch
          timeout: 5   //timout till the time it try to load else give exception
          clone-on-start: true   //clone the config to local at startring of app
          force-pull: true //it will pull the config and override the old local configs
-----------------------------------------------------------Encyption and decription -----------------------
we only need to include one property in our .properties file and /encrypt and /decrypt api will be expose which will be post call, it will take the values in plane text
body and return the asked result.

curl --location 'http://localhost:8071/decrypt' \
--header 'Content-Type: text/plain' \
--data 'c8504324c65a2c6cb4318bef424c2cc5e788be38afbb39b7b59112cf58c290525780edfa761f801bcf7bb17ef4a2d61f'


encrypt:
  key: "45D81EC1EF61DF9AD8D3E5BB397F9"   //This can be any complext text

while passing the values from propertie file we should pass (cipher) before the encrypted values of properties out server will understand it is encrypted and
it will do the decryption.

										-------------. Refresh configurations at runtime using refresh actuator path--------------
we need to add refresh endpoint in .properties file so, every time if there is any change in the config, it will be call /refresh endpoint and load the updated config.{dependecny need to add in ms}

management:
  endpoints:
    web:
      exposure:
        include: "*" //this is enabling all endpoint we can "/actuator" if want for this only. in all ms

====>LIMITATION-for every new change we need to call refresh endpoint maually  but this will overhead the flow, we need to comeup with new solution:-

						--- Refresh config at runtime using Spring Cloud Bus & Spring Cloud Config monitor---
Inside Spring Boot apps, actuator gathers the "Liveness" and "Readiness" information from the
ApplicationAvailability interface and uses that information in dedicated health indicators:
LivenessStateHealthIndicator and ReadinessStateHealthIndicator. These indicators are shown on
the global health endpoint ("/actuator/health"). They are also exposed as separate HTTP Probes
by using health groups: "/actuator/health/liveness" and "/actuator/health/readiness"

						------------------COmmonm -docker compose file----
services:
  rabbit:
    image: rabbitmq:3.13-management
    hostname: rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    healthcheck:
      test: rabbitmq-diagnostics check_port_connectivity
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 5s
    extends:
      file: common-config.yml  //common config which is neeed for alll the services
      service: network-deploy-service

  configserver:
    image: "eazybytes/configserver:s6"
    container_name: configserver-ms
    ports:
      - "8071:8071"
    depends_on:   //this will give the seq once dependent will be sucess.
      rabbit:
        condition: service_healthy
    healthcheck:
      test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    extends:
      file: common-config.yml
      service: microservice-base-config

  accounts:
    image: "eazybytes/accounts:s6"
    container_name: accounts-ms
    ports:
      - "8080:8080"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "accounts"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

  loans:
    image: "eazybytes/loans:s6"
    container_name: loans-ms
    ports:
      - "8090:8090"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "loans"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

  cards:
    image: "eazybytes/cards:s6"
    container_name: cards-ms
    ports:
      - "9000:9000"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "cards"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

networks:
  eazybank:
    driver: "bridge"



										----Service Discovery and Service registration----------
These challenges in microservices can be solved using below concepts or solutions,
1) Service discovery->Each instance of a microservice(unique ip)
exposes a remote API with it's own host and port. how do other microservices & clients know about these dynamic endpoint URLs to invoke them. So where is my servicÐµ?

2) Service registration=>If an microservice instance fails, new instances will be brought online to ensure constant availability. This means that the IP addresses of the
instances can be constantly changing.So how does these new instances can start serving to the clients?

3) Load balancing=>How do we make sure to properly load balance b/w the multiple microservice instances(each has own ip} especially a microservice is invoking another microservice? How do a specific
service information shared across the network?

=>The biggest challenge with traditional load balancers is that some one has to manually maintain the routing tables which is an impossible task inside the microservices 
network. Because containers/services are ephemeral in nature

For cloud native applications, service discovery is the perfect solution. It involves tracking and storing information about allrunning service instances in a service 
registry.Whenever a new instance is created, it should be registered in theregistry, and when it is terminated, it should be appropriately removed automatically.
The registry acknowledges that multiple instances of the same application can be active simultaneously. When an application needs
to communicate with a backing service, it performs a lookup in the registry to determine the IP address to connect to. If multiple
instances are available, a load-balancing strategy is employed to evenly distribute the workload among them.
Client-side service discovery and server-side service discovery are distinct approaches that address
the service discovery problem in different contexts


								-------------How it solve the problem---------
In a modern microservice architecture, knowing the right network location of an application is a much
more complex problem for the clients as service instances might have dynamically assigned IP addresses.
Moreover the number instances may vary due to autoscaling and failures.
Microservices service discovery & registration is a way for applications and microservices to locate each
other on a network. This includes,
A central server (or servers) that maintain a global view of addresses
Microservices/clients that connect to the central server to register their address when they start & ready
Microservices/clients need to send their heartbeats at regular intervals to central server about their health
Microservices/clients that connect to the central server to deregister their address when they are about to shutdown


Microservices service discovery & registration is a way for applications and microservices to locate each
other on a network. This includes,
A central server (or servers) that maintain a global view of addresses
Microservices/clients that connect to the central server to register their address when they start & ready
Microservices/clients need to send their heartbeats at regular intervals to central server about their health
Microservices/clients that connect to the central server to deregister their address when they are about to shutdown

								------------Client-side service discovery and load balancing bytes------
In client-side service discovery, applications are responsible for registering themselves with a service registry during startup and unregistering
when shutting down. When an application needs to communicate with a backing service, it queries the service registry for the associated IP
address. If multiple instances of the service are available, the registry returns a list of IP addresses. The client application then selects one based
on its own defined load-balancing strategy
==>when we want to communitate with any upstream service, first our app will lookup the service-discouvery and get the list of ip address where upstream application instance
are running, once we got the list of ip then our load balacning stretgy will decide the which ip to use for communication, here discovery of service and ip is own by
client applicaiton, so, this is called client side serive discovery.

=>In the client side when we do first query of ip addresses, it will caches the list of ip address so, it won't over burden our discovery server for every api call, it will
get refesh in every few time to udpate the removed and added services to discouvery server, these all happens autometically using spring cloud.

=>The major advantage of client-side service discovery is load balancing can be implemented
using various algorithms, such as round-robin, weighted round-robin, least connections, or
even custom algorithms. A drawback is that client service discovery assigns more
responsibility to developers. Also, it results in one more service to deploy and maintain (the
service registry). Server-side discovery solutions solve these issues. We are going to discuss
the same when we are talking about Kubernetes
