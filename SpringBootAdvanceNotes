
https://github.com/eazybytes/microservices
PDF need for revision
quesiton:
ENtity vs DTO.
DTO-https://martinfowler.com/eaaCatalog/dataTransferObject.html
Auditing and transacitons in spring data jpa:-
https://medium.com/@lavishj77/spring-data-jpa-auditing-and-transaction-management-b9e3246c2968
Documentation of Rest api:-we need add dependency of springdoc-openapi and it will add the swagger 
document of our apis through swagger endpoint and we can add more details using annotation from this dependencies.
                            -:Strangler pattern for microservice migration:-
https://www.geeksforgeeks.org/system-design/strangler-pattern-in-micro-services-system-design/

                                                          ----Docker ------
when we are using cloud services, cloud service providers have their physical infar at their data centers called servers but those are huge infra so, they can
not be given to single org .so, with help of Hypervisior concept they divide the servers resouces like ram, memory etc viutually into different parts called virtual
Matchine(VM), these vms are work as computer having memomory,os etc, so, we can use these vms according to our need of our application .
->Suppose we want to run our one of the microservices to the vm so, we need to install java version needed, data base and other dependencies so,we can run our serivce but
there is one issue with it,its not sure that our service will need all the resouces of the vm , in that case it will do the over price and for n number of services
we need to have same number of vm which is not a good practice. neither we can deploye all our services to same vm , that might cause resouces issue, dependencies issue
like java version,diff db etc.similerly scaling will be required manual intervention so, this way vm are not right for our microservices.

  VM
vm(os,libranry etc)
Hypervisior
Servers

solution of this problem is Container->its name suggest it contains all the required dependencies for particular service(simile ship container contains all needed resouces
in it for like aaple container will have ac in it)

COntainers
container1(lib,dependecies)
container engine(Docker)
Host OS
Servers

=>Even os is not there in container so,its light weight  and its super quick to restart  and destroy.
->WE will create image of our container which will be representation of our container, we can create any no of container from image like any no of obj we can create from
java class.
->Docker container is running repsentation of running image.

Follow notes from -file:///C:/Users/abhijeetkumar5/Desktop/LearningNotes-main/Learning/java%20interview%20questions/Advance%20spring%20boot/Master+Microservices+with+SpringBoot,Docker,Kubernetes.pdf


                                                                      Redis
download and unzip and start server, it will ready for accpeting traffic.
:-https://github.com/microsoftarchive/redis/releases/tag/win-3.2.100
Redis (which stands for REmote DIctionary Server) is an open-source, in-memory data structure store that is used as a database, cache, and message broker. 
Because all data resides in main memory (RAM), it delivers extremely high performance with sub-millisecond latency. 
=>https://github.com/Java-Techie-jt/spring-data-redis/tree/main =>Redis as in memory db
                                                      Redis for caching
https://github.com/Java-Techie-jt/spring-data-redis-cache
when  @GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product") added at the method of serive or controller layer, first search take the data from db but next it took it from cache,
we find diff in time of response in the postman along with log of calling db is not there when we hit sencond timee.
 @Cacheable(key = "#id",value = "product") //Id is the key of cache and value is the hash of cachedb.

=>@GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product",unless = "#result.price>1000") //here product will be cache based on condition if price is less than 1000 only.
=>When we are deleting the data from db, we will have remove that data from cache too, here we do so:-

@DeleteMapping("/{id}")
    @CacheEvict(key="#id",value = "product")// key and value would same as we using in cacheable. it will remove the data from cache.
 @CachePut(value = "users", key = "#user.id")=> THis will update the the data to cache when we add new data to user.

-----------------------------------------------------------------------Creating image of our application using docker-------------------------------------
First we have to mention the packing of our application in pom.xml file at place where name and verion of applicaiton is mentioned.
===>> <packaging>jar</packaging>
=>Before we make code changes for docker, when we build the application, we will getting the jar craeted for our app, which will have all code and lib but not java runtime
this is also called fat jar having big sizw e.g- accounts-0.0.1-SNAPSHOT.jar
This is created with ref of name from pom:-
<artifactId>accounts</artifactId>
<version>0.0.1-SNAPSHOT</version>
=>mvn spring-boot:run ---->Runnnig from cmd.
=>java -jar target/accounts-0.0.1-SNAPSHOT.jar ------------------->run spring boot using java
--------------------------------------------------------Creating Docker file --------------------
#Start with a base image containing Java runtime:tag like version of jdk image.Java is need for our application to run so, we are using java image which is there at dockerhub.
FROM openjdk:21-jdk-slim

# MAINTAINER instruction is deprecated in favor of using label
# MAINTAINER eazybytes.com
#Information around who maintains the image
LABEL "org.opencontainers.image.authors"="eazybytes.com"

# Add the application's jar to the image
//here we are copying our jar file from target folder which is generated post build to image(which has java and now our applicaiton is there with)

COPY target/accounts-0.0.1-SNAPSHOT.jar accounts-0.0.1-SNAPSHOT.jar

# execute the application, this is the same cmd which we used to run our app using java cmd , there was space in bwn but here commona
ENTRYPOINT ["java", "-jar", "accounts-0.0.1-SNAPSHOT.jar"]


--------------------------------------------------------------------Running docke image-------------------------------------------
First check if docker is running in our system, cmd:- docker version
=>Build docker image: ->docker build . -t easybyte/accounts:s4
format:-docker build path -t(tag) userId/nameOfapp:tag(version)
->When we build it from cmd, first it will download the openjdk from dockerhub and try to make the image by copying our jar file and jdk to it.
------------------------------------------------Running docker containere---------------
docker run -p 8080:8080 ak608333319/accounts:s4 :--p(port fisrt port is for our local system, second port is for docker to expose at 8080 , since docker run on 
isolated network, we can run same image on same port multiple at same time since, they are isolated so, every private network can run on same)
docker run -d -p 8080:8080 ak608333319/accounts:s4 //-d means runnig in detach mode so, other cmd can be fired form cmd while one is running
docker run -d -p 8081:8080 ak608333319/accounts:s4

Above two containers are runnig same image but on diff port  , so we have scaled the our app to two pod here.
We are using same port number for docker to expose for two diff pod is mained by docker call Port mapping .

`````````````````````````````````````````````````````````Buildpacks--------------
Buildpacks are a technology that automates the process of turning application source code into runnable container images, typically without the need for a Dockerfile.
They achieve this by automatically detecting the programming language and framework of an application, installing necessary dependencies,
and configuring the runtime environment.

we will add jar packing to the pom file:-
<packaging>jar</packaging>

we will add the image tag to the plugin section of the pom file:-

                 <configuration>
                    <image>
                        <name>eazybytes/${project.artifactId}:s4</name>
                    </image>
                </configuration>

=>Run the build cmd which will scan the whole project and build the image using all dependencies and we do not have to write the insturction to include
java veresion copy the  fat jar from tartet to our image etc.
=>This is required to run the docker in background.
=>mvn spring-boot:build-image  //Generating docker image form mvn.
==>RUnning the new image generated:-
docker images =>get the list of image
=>docker run -d -p 8090:9090 eazybytes/loans:s4 =>run the image with its name.
we will able to see image is running  on docker , we can hit the api from postman to validate too.

-------------------------------------------------------------------------Pushing docker image to DockerHub-----------

docker images
docker image push docker.io/ak608333319/accounts:s4 

//this will push the image like same we does in github, here we have already login from our account in docker desktop
so, cread will be taken from there and our image will be pushed to hub without explicity asked for creds.

we can see this in reps:-https://hub.docker.com/repository/docker/ak608333319/accounts/general //same image can be used in pipeline to deployed in cloud.
=>First delete the accounts container and image then we will try to pull the image from hub and run it.

Anyone want to use this they can pull it:-docker pull ak608333319/accounts:s4
=>ocker run -d -p 8080:8080 ak608333319/accounts:s4 //running the image pulled from dockerhub
                                                    ----docker compose--------
Docker Compose is a tool for defining and running multi-container Docker applications. It simplifies the orchestration of multiple services that work together to form a
complete application, such as a web server, a database, and a caching layer. 
Docker Compose uses a single YAML file (typically compose.yml or docker-compose.yml) to define all the services, networks, and volumes required for your application.
This centralizes the configuration and makes it easy to manage.
sample file:-we will maintain the herarchie of yml

services:                                //this is the root level where list of services will be mentioned 
  accounts:                              //Microservice level infos
    image: "eazybytes/accounts:s4"       //image of account serice
    container_name: accounts-ms          //name of container else random will be assigned as we saw in desktop(docker)
    ports:                               //port at which docker and our sevices will listen
      - "8080:8080"
    deploy:                              //here we gave limitation to our app which uses the resources of docker
      resources:
        limits:
          memory: 700m                   //max 700 mb will be used by account services
    networks:                            //as we know docker run in isolated network so, if our services need to communicated with each other we need to have common network
      - eazybank
  loans:
    image: "eazybytes/loans:s4"
    container_name: loans-ms
    ports:
      - "8090:8090"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
  cards:
    image: "eazybytes/cards:s4"
    container_name: cards-ms
    ports:
      - "9000:9000"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
networks:                    //here we are defining the common network where all our apps will be running, network name is easybank and bridge will be used as driver to communicate
  eazybank:
    driver: "bridge"


We can run the compose file with cmd so, all of the services image will start running together, we do not have to do it each and every service wise neither we
have to create docker file for each service.

CMD:->docker compose up -d {running in detach mode, it will download the all dependencies and start our image , we need to hit the cmd from the location where our compose file}
      //created containers
CMD->docker copose down -d {stopped and revmove the running image}, it will be removed we can check by doing docker ps.
CMD->docker compose start (start the stopped containers only, not removed containers)
CMD->docker compose stop (stop the the running container, if not running container it will give error message)


                                            ----Imp cmd of Docker--------
Docker Commands used in the course                       Docker Command	Description

"docker build . -t eazybytes/accounts:s4"   	            To generate a docker image based on a Dockerfile
"docker run -p 8080:8080 eazybytes/accounts:s4"	          To start a docker container based on a given image
"docker images"                                          	To list all the docker images present in the Docker server
"docker image inspect image-id"	                          To display detailed image information for a given image id
"docker image rm image-id"	                              To remove one or more images for a given image ids
"docker image push docker.io/eazybytes/accounts:s4"	      To push an image or a repository to a registry
"docker image pull docker.io/eazybytes/accounts:s4"	      To pull an image or a repository from a registry
"docker ps"		                                            To show all running containers
"docker ps -a"		                                        To show all containers including running and stopped
"docker container start container-id"		                  To start one or more stopped containers
"docker container pause container-id"	                  	To pause all processes within one or more containers
"docker container unpause container-id"		                To unpause all processes within one or more containers
"docker container stop container-id"		                  To stop one or more running containers
"docker container kill container-id"		                  To kill one or more running containers instantly
"docker container restart container-id"	                  To restart one or more containers
"docker container inspect container-id"		                  To inspect all the details for a given container id
"docker container logs container-id"		                  To fetch the logs of a given container id
"docker container logs -f container-id"		                  To follow log output of a given container id
"docker container rm container-id"		                  To remove one or more containers based on container ids
"docker container prune"		                              To remove all stopped containers
"docker compose up"		                 	                   Creates and starts containers based on the given Docker Compose file
"docker compose down"		                 	                   Stops and removes containers, networks, volumes, and images created by up
"docker compose start"		                                Starts existing (previously created) containers without recreating them
"docker compose stop"		                                  Stops running containers without removing them
"docker run -p 3306:3306 --name accountsdb -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=accountsdb -d mysql"        	        To create a MySQL DB container
"docker run -p 6379:6379 --name eazyredis -d redis"        	To create a Redis Container
"docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:22.0.3 start-dev"	To create Keycloak Container



                                                                            DOcker extention
Docker Extensions are add-ons that integrate third-party tools and functionalities directly into Docker Desktop, enhancing its capabilities. 
They allow users to seamlessly connect their preferred development tools to their application development and deployment workflows. 
 section 4 code:-https://github.com/eazybytes/microservices/tree/3.4.1/section4

Cloud native application:-
A cloud-native application is a software application designed and built specifically to run in a cloud environment, taking full advantage of its scalability,
flexibility, and resilience. These applications are built using a microservices architecture, where functionality is broken down into small, independent services.
Key technologies include containerization (like Docker and Kubernetes), CI/CD pipelines, and serverless functions. 
cloud native app can be on gcp,aws,azure etc.
Key characteristics

Agile development: They support agile development practices, which allows for rapid deployment of new features and faster time-to-market. 
Scalability and resilience
Automation:ci/cd
Containerization:docker or othe tool
Microservices architecture:

                                                =======12 factos and 15 factor Methodology============
The "15-Factor Methodology" is a set of principles for developing modern, cloud-native applications, an extension of the original "Twelve-Factor App" methodology. 
The 15 factors are:
1.One Codebase, one Application : Each application has only one code base which will be independently deployeing using packing (build once) and run in multiple env
we should have config of diff env externally injected during deployement, so, once its build will be running same in all env.Its easy to maintain and tack.

2.API First: Applications should be designed with an API-first approach, defining service contracts upfront using a language like Open API Specification (Swagger).
dictates that applications should be designed with their Application Programming Interfaces (APIs) as "first-class citizens" from the outset of the development process.
This means the API contract and design are defined and agreed upon before writing any application code or building user interfaces. 

3.Dependency Management: Explicitly declare and isolate dependencies using a dependency declaration manifest and an isolation tool.we use maven or gradle where we declr
our dependencies which will be download and mangaed by it, we don't need to download explicitely.

4.Design, Build, Release, Run: Strictly separate the build, release, and run stages to create an immutable build artifact at each release.All setsps shold be followed and 
does not  make any changes at any stage , so make it immutable. 

5.Config: Store configuration that varies between environments (staging, production, etc.) in environment variables or external configuration services, never in the code.

6.Logs: Treat logs as event streams, directing them to standard output and leaving the aggregation and storage to external tools.so, we can see the logs of all the MS at single place.

7.Disposability: Maximize robustness by designing processes that can be started or stopped at a moment's notice with fast startup and graceful shutdown.In the cloud native
we have docker which make our application to light image which will be easy to deploye and stopped if needed , while stopping our aaplicatin should not take more req and 
complete the ongoing req, in case of neeed it will upscale and downsacle autometically, if somthing went wrong it will be restart the app, these capablity is acheive by
kubernative, so,

8.Backing Services: Treat all backing services (databases, message brokers, caching systems, etc.), whether local or third-party, as attached resources accessed via a URL/credentials stored in the config.

9.Environment Parity: Keep development, staging, and production environments as similar as possible to minimize gaps in time, people, and tools.

10.Administrative Processes: Run admin/management tasks (database migrations, batch jobs, etc.) as one-off processes in an identical environment to the regular long-running app processes.
Thease should be treated as isolated process and deplouyed in each env.we should deployed thease as seperate MS.

11.Port Binding: Applications should be self-contained and expose their services via port binding, not relying on runtime injection of a web server.

12.Stateless Processes: Execute the app as one or more stateless and share-nothing processes, storing any necessary persistent data in a stateful backing service.
should not store any data in session or any where expect db,it won't have same instance in next req, so, all instance are isolated to each other.

13.Concurrency: Scale out via the process model, designing applications to distribute workload across multiple processes.
In java app JVM handle this multi threading by using thread from thread pool, by default we have 200 thread avail to serve the client.

14.Telemetry: Design the application to include the collection of monitoring data, health information, and statistics (APM, domain-specific data, system logs).

25.Authentication & Authorization: Implement robust security measures, especially for APIs, using solutions like OAuth2 or OpenID Connect to handle user authentication and authorization. 

                                      ==========Configuration Management ============
If we bind all code and config to the application, we need to build the image in every other env since, our config is hard binded to code, so this process would be not be
good practice, so, config should be externally managed and once our code image is build can be used in all env , out config would be inject at the time of deployment
it would be env wise so, we will be free from overhead of config.

=>Spring boot let us externalise the configuration so, we can use the same code base in all env .
=>Bydefault Spring boot look for config in application.properties, it has set of priority which will override the configs;-
application .properties<os Env veriable<java system properties(System.getProperties())<JIndi components<Servelet init paramenters<servelet config parameters<command line arg.

=>we can read the config injava using 
1.@value , it will read one value at time
@Value("${build.version}")
    private String buildVersion;

2.Environement interface->env.getProperties() one config at time, this is used to read the env veriable like secret,password;
@Autowired
private Environment environment;
 @GetMapping("/java-version")
    public ResponseEntity<String> getJavaVersion() {
        return ResponseEntity
                .status(HttpStatus.OK)
                .body(environment.getProperty("JAVA_HOME"));//this will return the path of our java insalled in system.
    }

3.@ConfigurationProperties("prefix")->we can define the pojo and it will read the all config with specified prefix to a pojo.

confog from yml:-
accounts:
  message: "Welcome to EazyBank accounts related local APIs "
  contactDetails:
    name: "John Doe - Developer"
    email: "john@eazybank.com"
  onCallSupport:
    - (555) 555-1234
    - (555) 523-1345

Reading into pojo of Record class:-
@ConfigurationProperties(prefix = "accounts")
public record AccountsContactInfoDto(String message, Map<String, String> contactDetails, List<String> onCallSupport) {
//THe dto will be populated during starting of app.
}
@EnableConfigurationProperties(value = {AccountsContactInfoDto.class}) we need to add this to our application class.

The @EnableConfigurationProperties annotation can be declared in two primary places:
On the main application class (the class annotated with @SpringBootApplication).
On a separate @Configuration class. 
How Does It Work?
@ConfigurationProperties: This annotation defines a class where external configuration properties will be mapped. You can specify a prefix that matches the keys in your application.properties or application.yml file.
@EnableConfigurationProperties: This annotation is applied to a Spring configuration class and specifies which configuration properties classes should be bound from external configuration sources.
                                                   
E.g-
@ConfigurationProperties(prefix = "app")
public class AppProperties {
    private String name;
    private String version;
}

@Configuration
@EnableConfigurationProperties({AppProperties.class, DatabaseProperties.class})
public class AppConfig {
}

                                                                                  ------Profiles-----
=>Spring provides a great tool for grouping configuration properties into so-called profiles(dev, qa, prod) allowing us to activate a bunch of configurations based on the active
profile.
=>The default profile is always active. Spring Boot loads all properties in application.properties into the default profile.We can create another profiles by creating property files like below,
=>application_prod.properties ---------> for prod profile
=>application_qa.properties ---------> for QA profile
=>We can activate a specific profile using spring.profiles.active property like below,
spring.profiles.active=prod
=>When we have common properties which are used by all env , we will keep in application.properties/yml file and env specific config would be there inside the env files.
e.g for prod:-

spring:
  config:
    activate:
      on-profile: "prod"  //similer in each yml file for marking it for that env

TO Activate that env we will use this config in application.yml file:-
Spring:
  config:
    import:
      - "application_qa.yml"  //we will import all config files to default config
      - "application_prod.yml"
  profiles:
    active:    //activating the specific env profile.
      - "qa"
=>We can externalize the configuration so, we can update the profile externally, we can do it by USing Command Line Argument from Inetlij, program arg from run config:-
--spring.profile.active="qa" --build.version="1.1"   //we will use -- to overide the properties file values
=>With Help of VM argument:-Dbuild.version="1.2" -Dspring.profile.active="qa"  //-D prefix is added to the profperties
=>ENV veriable form config:BUILD_VERION=1.3;SPRING_PROFILE_ACTIVE=qa  //all key would be in capital and . will be replace with _
priority->cmd-arg>jvm arg> file config

                              -----Drawbacks of externalized configurations using SpringBoot alone -----
challenges of using standard methods like CLI arguments, JVM properties, and environment variables for managing application configuration,
and poses six specific questions regarding advanced configuration management strategies.
=>THis req manual intervention, service restart,Admin can see the password or cred used in these args, so, this is not full proof solution for externalization of config.

                                          =============Spring Cloud Config =============
Spring Cloud Config provides server and client-side support for externalized configuration in a distributed system. With the Config Server you have a central place to 
manage external properties for applications across all environments. The concepts on both client and server map identically to the Spring Environment and PropertySource 
abstractions, so they fit very well with Spring applications, but can be used with any application running in any language. As an application moves through the deployment
pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run
when they migrate. The default implementation of the server storage backend uses git so it easily supports labelled versions of configuration environments, as well as being 
accessible to a wide range of tooling for managing the content. It is easy to add alternative implementations and plug them in with Spring configuration.

Features
Spring Cloud Config Server features:

HTTP, resource-based API for external configuration (name-value pairs, or equivalent YAML content)
Encrypt and decrypt property values (symmetric or asymmetric)
Embeddable easily in a Spring Boot application using @EnableConfigServer
Config Client features (for Spring applications):

Bind to the Config Server and initialize Spring Environment with remote property sources
Encrypt and decrypt property values (symmetric or asymmetric)

@EnableConfigServer->Mark the service as Config server

=>will have cloud config dependencies. we will put all the configs of loan,account,card services to the resources of this service which will be used to provide to its client
services which they are.Each env and service config would be there in config folder of the server service.

                                                                      ----application.yml file of cloud config server MS:-

spring:
  application:
    name: "configserver"   //name of service
  profiles:
    active: native         //we use native in case of file is stored in classpath
  cloud:
    config:
      server:
        native:
          search-locations: "classpath:/config"  //location where all configs are stored
server:
  port: 8071


===>When we try to run the application, we can get the all the configuration using sample endpoint:-http://localhost:8071/accounts/prod
accounts-prod.yml->file name present in config folder of classpath.
=>With this name:-accounts.yml->http://localhost:8071/accounts/default , this will return default properties.
=>Now our config server is rady to serve the config through endpoint.
=>update the server file location to:-search-locations: "file:///Users//eazybytes//Documents//config"

                          ---------------------Client application for Accounts service--------------
=>yml file for client service :-
server:
  port: 8080
spring:
  application:
    name: "accounts"   //name of the service will be used to req the config from the server.
  profiles:
    active: "prod"      //this will be used to load the profile of the config-name passed from clint to server
  config:
    import: "optional:configserver:http://localhost:8071/"  //this wil load the config server which is running at port 8071, according to name and profile given here 
														//properties will be loaded to our ms, optional is something which will be ignore and run our applicatin in case server conntion failed .


we need to add config dependencies:-
<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-config</artifactId>
		</dependency>

version:-<spring-cloud.version>2024.0.0</spring-cloud.version>
sme need to add in dependencies management:-
<dependencyManagement>
		<dependencies>
			<dependency>
				<groupId>org.springframework.cloud</groupId>
				<artifactId>spring-cloud-dependencies</artifactId>
				<version>${spring-cloud.version}</version>
				<type>pom</type>
				<scope>import</scope>
			</dependency>
		</dependencies>
	</dependencyManagement>

											------Reading configs from file system in config-server----------
==>when we will run accounts services, we will get the data from config server for prod:-http://localhost:8080/api/contact-info
->Here we are readiing the config from .properties file using 
@EnableConfigurationProperties(value = {AccountsContactInfoDto.class})   //Main class level
@ConfigurationProperties(prefix = "accounts")  //DTO class level.
											---Reading config from Git---

spring:
  application:
    name: "configserver"
  profiles:
     active: git    //to read the config from git
  cloud:
    config:
      server:
        git:  
          uri: "https://github.com/eazybytes/eazybytes-config.git"  //repo where configs are placed
														=>We can read it from private git or other place, we can refer the offical doc for other way to read the configs.
          default-label: main   //branch
          timeout: 5   //timout till the time it try to load else give exception
          clone-on-start: true   //clone the config to local at startring of app
          force-pull: true //it will pull the config and override the old local configs
-----------------------------------------------------------Encyption and decription -----------------------
we only need to include one property in our .properties file and /encrypt and /decrypt api will be expose which will be post call, it will take the values in plane text
body and return the asked result.

curl --location 'http://localhost:8071/decrypt' \
--header 'Content-Type: text/plain' \
--data 'c8504324c65a2c6cb4318bef424c2cc5e788be38afbb39b7b59112cf58c290525780edfa761f801bcf7bb17ef4a2d61f'


encrypt:
  key: "45D81EC1EF61DF9AD8D3E5BB397F9"   //This can be any complext text

while passing the values from propertie file we should pass (cipher) before the encrypted values of properties out server will understand it is encrypted and
it will do the decryption.

										-------------. Refresh configurations at runtime using refresh actuator path--------------
we need to add refresh endpoint in .properties file so, every time if there is any change in the config, it will be call /refresh endpoint and load the updated config.{dependecny need to add in ms}

management:
  endpoints:
    web:
      exposure:
        include: "*" //this is enabling all endpoint we can "/actuator" if want for this only. in all ms

====>LIMITATION-for every new change we need to call refresh endpoint maually  but this will overhead the flow, we need to comeup with new solution:-

						--- Refresh config at runtime using Spring Cloud Bus & Spring Cloud Config monitor---
Inside Spring Boot apps, actuator gathers the "Liveness" and "Readiness" information from the
ApplicationAvailability interface and uses that information in dedicated health indicators:
LivenessStateHealthIndicator and ReadinessStateHealthIndicator. These indicators are shown on
the global health endpoint ("/actuator/health"). They are also exposed as separate HTTP Probes
by using health groups: "/actuator/health/liveness" and "/actuator/health/readiness"

						------------------COmmonm -docker compose file----
services:
  rabbit:
    image: rabbitmq:3.13-management
    hostname: rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    healthcheck:
      test: rabbitmq-diagnostics check_port_connectivity
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 5s
    extends:
      file: common-config.yml  //common config which is neeed for alll the services
      service: network-deploy-service

  configserver:
    image: "eazybytes/configserver:s6"
    container_name: configserver-ms
    ports:
      - "8071:8071"
    depends_on:   //this will give the seq once dependent will be sucess.
      rabbit:
        condition: service_healthy
    healthcheck:
      test: "curl --fail --silent localhost:8071/actuator/health/readiness | grep UP || exit 1"
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    extends:
      file: common-config.yml
      service: microservice-base-config

  accounts:
    image: "eazybytes/accounts:s6"
    container_name: accounts-ms
    ports:
      - "8080:8080"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "accounts"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

  loans:
    image: "eazybytes/loans:s6"
    container_name: loans-ms
    ports:
      - "8090:8090"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "loans"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

  cards:
    image: "eazybytes/cards:s6"
    container_name: cards-ms
    ports:
      - "9000:9000"
    depends_on:
      configserver:
        condition: service_healthy
    environment:
      SPRING_APPLICATION_NAME: "cards"
    extends:
      file: common-config.yml
      service: microservice-configserver-config

networks:
  eazybank:
    driver: "bridge"



										----Service Discovery and Service registration----------
These challenges in microservices can be solved using below concepts or solutions,
1) Service discovery->Each instance of a microservice(unique ip)
exposes a remote API with it's own host and port. how do other microservices & clients know about these dynamic endpoint URLs to invoke them. So where is my servicÐµ?

2) Service registration=>If an microservice instance fails, new instances will be brought online to ensure constant availability. This means that the IP addresses of the
instances can be constantly changing.So how does these new instances can start serving to the clients?

3) Load balancing=>How do we make sure to properly load balance b/w the multiple microservice instances(each has own ip} especially a microservice is invoking another microservice? How do a specific
service information shared across the network?

=>The biggest challenge with traditional load balancers is that some one has to manually maintain the routing tables which is an impossible task inside the microservices 
network. Because containers/services are ephemeral in nature

For cloud native applications, service discovery is the perfect solution. It involves tracking and storing information about allrunning service instances in a service 
registry.Whenever a new instance is created, it should be registered in theregistry, and when it is terminated, it should be appropriately removed automatically.
The registry acknowledges that multiple instances of the same application can be active simultaneously. When an application needs
to communicate with a backing service, it performs a lookup in the registry to determine the IP address to connect to. If multiple
instances are available, a load-balancing strategy is employed to evenly distribute the workload among them.
Client-side service discovery and server-side service discovery are distinct approaches that address
the service discovery problem in different contexts


								-------------How it solve the problem---------
In a modern microservice architecture, knowing the right network location of an application is a much
more complex problem for the clients as service instances might have dynamically assigned IP addresses.
Moreover the number instances may vary due to autoscaling and failures.
Microservices service discovery & registration is a way for applications and microservices to locate each
other on a network. This includes,
A central server (or servers) that maintain a global view of addresses
Microservices/clients that connect to the central server to register their address when they start & ready
Microservices/clients need to send their heartbeats at regular intervals to central server about their health
Microservices/clients that connect to the central server to deregister their address when they are about to shutdown


Microservices service discovery & registration is a way for applications and microservices to locate each
other on a network. This includes,
A central server (or servers) that maintain a global view of addresses
Microservices/clients that connect to the central server to register their address when they start & ready
Microservices/clients need to send their heartbeats at regular intervals to central server about their health
Microservices/clients that connect to the central server to deregister their address when they are about to shutdown

								------------Client-side service discovery and load balancing bytes------
In client-side service discovery, applications are responsible for registering themselves with a service registry during startup and unregistering
when shutting down. When an application needs to communicate with a backing service, it queries the service registry for the associated IP
address. If multiple instances of the service are available, the registry returns a list of IP addresses. The client application then selects one based
on its own defined load-balancing strategy
==>when we want to communitate with any upstream service, first our app will lookup the service-discouvery and get the list of ip address where upstream application instance
are running, once we got the list of ip then our load balacning stretgy will decide the which ip to use for communication, here discovery of service and ip is own by
client applicaiton, so, this is called client side serive discovery.

=>In the client side when we do first query of ip addresses, it will caches the list of ip address so, it won't over burden our discovery server for every api call, it will
get refesh in every few time to udpate the removed and added services to discouvery server, these all happens autometically using spring cloud.

=>The major advantage of client-side service discovery is load balancing can be implemented
using various algorithms, such as round-robin, weighted round-robin, least connections, or
even custom algorithms. A drawback is that client service discovery assigns more
responsibility to developers. Also, it results in one more service to deploy and maintain (the
service registry). Server-side discovery solutions solve these issues. We are going to discuss
the same when we are talking about Kubernetes

									-------Spring Cloud support for Client-side service discovery---
Spring Cloud project makes Service Discovery & Registration setup trivial to undertake with the help of the below components,
->Spring Cloud Netflix's Eureka service which will act as a service discovery agent
->Spring Cloud Load Balancer library for client-side load balancing
->Netflix Feign client to look up for a service b/w microservices
Though in this course we use Eureka since it is mostly used but they are other service
registries such as etcd, Consul, and Apache Zookeeper which are also good.
Though Netflix Ribbon client-side is also good and stable product, we are going to use Spring
Cloud Load Balancer for client-side load balancing. This is because Ribbon has entered a
maintenance mode and unfortunately, it will not be developed anymore.

Advantages of Service Discovery approach includes,
No limitations on availability
Peer to peer communication b/w Services Discovery agents
Dynamically managed IPs, configurations & Load balanced
Fault-tolerant & Resilient in nature=
=>https://netflixtechblog.com/netflix-oss-and-spring-boot-coming-full-circle-4855947713a0
										====Setup Service Discovery agent using Eureka==={Section 8}
Fisrt we need to start the spring cloud server to get the properties

e.g-http://localhost:8071/eurekaserver/default

{
  "name": "eurekaserver",
  "profiles": [
    "default"
  ],
  "label": null,
  "version": "5acacd724617ed0afebb955a92b3ac13ac96dfdf",
  "state": "",
  "propertySources": [
    {
      "name": "https://github.com/eazybytes/eazybytes-config.git/eurekaserver.yml", //our prop file for eureka server
      "source": {
        "server.port": 8070,
        "eureka.instance.hostname": "localhost",
        "eureka.client.fetchRegistry": false, //do not fetch the registry details since, its server service, it does by client service
        "eureka.client.registerWithEureka": false, //same for this propr
        "eureka.client.serviceUrl.defaultZone": "http://${eureka.instance.hostname}:${server.port}/eureka/"  //expose endpoint
      }
    }
  ]
}
->Dependecies for eureka server MS:-
spring-cloud-starter-netflix-eureka-server->Mark this service as eureka server
spring-cloud-starter-config->THis will use to consume the configs from configserver api
spring-boot-starter-actuator->For health check
@EnableEurekaServer->This will enabled the MS as Eureka server.This will be on main class level annotation.

Properties file:-
spring:
  application:
    name: "eurekaserver"
  config:
    import: "optional:configserver:http://localhost:8071/"

management:
  endpoints:
    web:
      exposure:
        include: "*"
  health:
    readiness-state:
      enabled: true
    liveness-state:
      enabled: true
  endpoint:
    health:
      probes:
        enabled: true

we can start our service and we will get the server Dashboard where we can see all the services :-
check dashboard:-http://localhost:8070/
										---Client MS to register with EUREKA server--
Dependencies:-spring-cloud-starter-netflix-eureka-client
when we connect with server, our client service will register it self with server and send the heartbeat in every 30 sec default.we can see the details of client service
on the Server dashboard.
@EnableFeignClients:-In main method to enabled our service as client.

 Properties file:-

server:
  port: 8080
spring:
  application:
    name: "accounts"
  profiles:
    active: "prod"
  config:
    import: "optional:configserver:http://localhost:8071/"

management:
  endpoints:
    web:
      exposure:
        include: "*"
  endpoint:						//acutuator endpoint to shutdown post deregistering
    shutdown:
      enabled: true
  info:							//info level endpoint enabled to expose the info from this file
    env:
      enabled: true

endpoint:						//on root level, to call the shutdown endpoint
    shutdown:
      enabled: true

eureka:
  instance:
    preferIpAddress: true		//service will use this service ip to communitcate
  client:
    fetchRegistry: true         //registrying as client
    registerWithEureka: true   //
    serviceUrl:
      defaultZone: http://localhost:8070/eureka/   //Url where our service will get register

info:
  app:
    name: "accounts"
    description: "Eazy Bank Accounts Application"
    version: "1.0.0"
->our service details will be available on dahboard and if we client on it this will be called:-http://172.23.176.1:8080/actuator/info

we can see actuator info endpint is being called which we have enabled and ip address is being used since, we have enabled and info details are shown which we have
defined in properties file.
->Eureka expose its endpoint which share all the information of clients e.g:-
http://localhost:8070/eureka/app->This will share all the info of server registered apps
http://localhost:8070/eureka/apps/LOANS->This will give the all the infos related to LOANS api only, we can pass Accept->application/json in header , we will get the
info in json format. apps/service-name . 

								---------------------------Dregister from server------------------
we have already given the shutdown endpoint expose through the actuator, which will make the post call and stop the applicaiton , so, it will de resiter then shoutdown
and this will removed our app from discovery server.
->Heart beat is send by our applicaiton to discovery server in every 30 sec using put call, if we stop the discovery server, we can see erro in ms so, they are not able
to send the heart beat.

									======== Feign Client code changes to invoke other microservices==========
when we use RestTemplate and webclient we used url,reqpayload,port etc but in caes of openFeign client we write declerative code , no need to write all implemenatation code here.

THis is need to declear in the client service to call the any services using feign client:-

@FeignClient("cards") //on interface level and name should same which is being used by card service to register with discovery server
public interface CardsFeignClient {

    @GetMapping(value = "/api/fetch",consumes = "application/json")                  // same method signature which method neeed to invoke from card  controller,only declearation will be used to call,
    public ResponseEntity<CardsDto> fetchCardDetails(@RequestParam String mobileNumber);  //rest impl will be taken care of feign client.


}
ResponseEntity<CardsDto> cardsDtoResponseEntity = cardsFeignClient.fetchCardDetails(mobileNumber);
//when we try to call agaist the bean of CardsFeignClient, we will get the response from card service , feign will take care of service discovery, loadbalancing and 
service call impl too. we will get the deteails like rest api call.

All things r taken care by Eureka server and feign client , developer need not to worry about the implementation.

							------------Eureka Self-Preservation mode to avoid network trap issues-----

when client services does not send the heart beat to the server , discovery server will remove that instance form registry but it will follow one threshold like if 
particular number of heart beat is not being recive for no of instances register , then in that case it will not remove the instanec, instied it will go to serlf-preservation
mode so, it will keep the instance even not getting the heart beat , this it does by assuming there would be some network or internet issue, this will keep the instance live.
server will start gettng the heart beat once, its fixed the issue, we can controll these using configuration.

->We can create docker image of all the MS and push to the dockerhub and same can be used by running the image from cmd and test the apis.

													----ROUTING, CROSS CUTTING CONCERNS IN MICROSERVICES-----------
HOW DO WE MAINTAIN A SINGLE ENTRYPOINT INTO MICROSERVICES NETWORK=> all client do not have keep the details for each services.
HOW DO WE HANDLE CROSS CUTTING CONCERNS?==>logs,audit,trace, security acrross all the microservices (same code can't be put for each services as libaraby, it will incr the coupling)
HOW DO WE ROUTE BASED ON USTOM REQUIREMENTS ==>difining routing rule based on header,reqParm inside the microservice netwok.
==
In a scenario where multiple clients directly
connect with various services, several
challenges arise. For instance, clients must
be aware of the URLs of all the services, and
enforcing common requirements such as
security, auditing, logging, and routing
becomes a repetitive task across all services.
To address these challenges, it becomes
necessary to establish a single gateway as
the entry point to the microservices
network.
==
solution is edge server(API GATEWAY)----------------------------------------------------

Spring Cloud Gateway features:

Built on Spring Framework and Spring Boot
Compatible with both Spring WebFlux and Spring Web MVC
Able to match routes on any request attribute.
Predicates and filters are specific to routes.
Spring Cloud Circuit Breaker integration.
Spring Cloud DiscoveryClient integration
Easy to write Predicates and Filters
Request Rate Limiting
Path Rewriting


													==========Spring Cloud Gateway================
The service gateway sits between all calls from the client to the individual services & acts as a central Policy Enforcement Point (PEP) like below, 
Routing (Both Static & Dynamic) ,Security (Authentication & Authorization) Logging, Auditing and Metrics collection protocol conversion.

													=========Spring Cloud Gateway internal architecture======
when req com from client it will land to =>Gateway Handler (Mapping using Routing config, it will have req mappin so, will decidie which endpint to hit)
=>Predicates (To check if the requests fulfill a set of given condition)==> Pre Filters( set of filter which is used to  to execute logic on a request before it is routed to a backend service)
=>then re will go to MS and post that we will have=>POST FILTER ->GATWWAY HANDLER->Response to Client.

												=============== Building Edge Server using Spring Cloud Gateway=========
														https://github.com/eazybytes/microservices/tree/3.4.1/section8
Dependencies:-Gateway,Eureka DIscovery client(to get the all services endpoint details from discovery server),config client,Actuator.
Properties file:-

server:
  port: 8072
eureka:
  instance:
    preferIpAddress: true
  client:
    registerWithEureka: true
    fetchRegistry: true
    serviceUrl:
      defaultZone: "http://localhost:8070/eureka/"
spring:
  application:
    name: "gatewayserver"
  config:
    import: "optional:configserver:http://localhost:8071/"  //to get the details of configs service properties
  cloud:
    gateway:
      discovery:
        locator:
          enabled: false
          lowerCaseServiceId: true
// This property is a flag that enables or disables the automatic creation of routes based on services registered with a DiscoveryClient.it it is false enabled:false
the Spring Cloud Gateway will not automatically generate routes for every service found in the service registry. You will have to define all your routes manually in
your configuration file or via Java code using a RouteLocator. 
=>To use Spring Cloud Gateway without a discovery server, you need to configure the routes statically, either in your application's properties file or using Java configuration. 
->we will start the service in the seq like config-server->eureka server->card,loand,account services->gateway service
->when we seee on the eureka server all servives will be registered, 
->http://localhost:8072/actuator (to see the actuator endpoint of gateway serviec) we will see the actuator gateway service is exposed:-
->"gateway": {
      "href": "http://localhost:8072/actuator/gateway",
      "templated": false
    }
=>http://localhost:8072/actuator/gateway/routes   //this endpoint will give us all the routing details of serviecs from actuator gateway endpoint.
e.g-
{
    "predicate": "Paths: [/eazybank/accounts/**], match trailing slash: true",
    "route_id": "16c16fd1-e9bc-4aea-a1d0-be418901d58f",
    "filters": [
      "[[RewritePath /eazybank/accounts/(?\u003Csegment\u003E.*) = '/${segment}'], order = 0]",  //here our path is being rewrite, any req with  /eazybank/accounts/ will be route to the resouce path only .
      "[[AddResponseHeader X-Response-Time = '2025-12-01T09:02:01.014064800'], order = 0]"
    ],
    "uri": "lb://ACCOUNTS",  //this will leverage the service discovery service named with ACCOUNTS and route the req using spring boot loadbalancer
    "order": 0
  }

==>we can call out accounts apis using gateway service:-

curl --location 'http://localhost:8072/ACCOUNTS/api/create' \     //8072->gateway port,ACCOUNTS ->name of the service registered to discovery server and lb of cloud service uses to call the service
--header 'Content-Type: application/json' \
--data-raw '{"name": "Abhijeet",
"mobileNumber": "98099998789",
"email": "ak980888@gmail.com"
}'

													---Define custom routes----
@Bean   //inside the main class
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()             //route builder will be used to create thebean of the route and it will mark the service as custom routes   
						.route(p -> p
								.path("/eazybank/accounts/**")    //Predicate , if it matches then it will move ahead.
								.filters( f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment}") //rewriitng the path to veriable segment using filter
										.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))  // filter for adding the Response header 
								.uri("lb://ACCOUNTS"))  //aading the spring cloud load balancer using service discovery app name;
					.route(p -> p
							.path("/eazybank/loans/**")
							.filters( f -> f.rewritePath("/eazybank/loans/(?<segment>.*)","/${segment}")
									.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
							.uri("lb://LOANS"))
					.route(p -> p
							.path("/eazybank/cards/**")
							.filters( f -> f.rewritePath("/eazybank/cards/(?<segment>.*)","/${segment}")
									.addResponseHeader("X-Response-Time", LocalDateTime.now().toString()))
							.uri("lb://CARDS")).build();


	}
							--------Implementing Cross cutting concerns Tracing & Logging using Gateway -----
we can write multiple filter to address cross cutting concerns like adding correlationid , logs etc, when we will have multiple filter we can add the priority by adding
@Order(1) annotation on the filter class.
Our custom filter will implement the GlobalFilter from gateway library and override the filter method

e.g-- adding correleation id to the requestFilter in case its not there.
@Order(1)
@Component
public class RequestTraceFilter implements GlobalFilter {

    private static final Logger logger = LoggerFactory.getLogger(RequestTraceFilter.class);

    @Autowired
    FilterUtility filterUtility;

    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        HttpHeaders requestHeaders = exchange.getRequest().getHeaders();
        if (isCorrelationIdPresent(requestHeaders)) {
            logger.debug("eazyBank-correlation-id found in RequestTraceFilter : {}",
                    filterUtility.getCorrelationId(requestHeaders));
        } else {
            String correlationID = generateCorrelationId();
            exchange = filterUtility.setCorrelationId(exchange, correlationID);
            logger.debug("eazyBank-correlation-id generated in RequestTraceFilter : {}", correlationID);
        }
        return chain.filter(exchange);
    }
=>we can add the correlation id to other services as requestHeader so, those can be used in all dependent service to main the logging pattern for debuging, we can enabled 
the logs using this config:-
logging:
  level:
    com:    //pakcage
      eazybytes: //subpakage
        accounts: DEBUG //subpakage

@RequestHeader("eazybank-correlation-id") String correlationId->This will be used in controller to pass this id to other services.
                                                                       
==>SImilerly we can add the response filter and the correlationid to the responseheader to get it ack at UI.
=>Here we can create the Global filter bean direclty using @bean not req to implements and overrid filter method(second way)
@Configuration
public class ResponseTraceFilter {

    private static final Logger logger = LoggerFactory.getLogger(ResponseTraceFilter.class);

    @Autowired
    FilterUtility filterUtility;

    @Bean
    public GlobalFilter postGlobalFilter() {
        return (exchange, chain) -> {
            return chain.filter(exchange).then(Mono.fromRunnable(() -> {
                HttpHeaders requestHeaders = exchange.getRequest().getHeaders();
                String correlationId = filterUtility.getCorrelationId(requestHeaders);
                logger.debug("Updated the correlation id to the outbound headers: {}", correlationId);
                exchange.getResponse().getHeaders().add(filterUtility.CORRELATION_ID, correlationId);
            }));
        };
    }
}


																	-----------API Gateway Pattern--------------
The API Gateway Pattern is a critical architectural component in microservices design, offering a unified entry point for multiple
microservices. It acts as a gateway between the external clients (e.g, web apps, mobile apps) and the internal microservices, helping
streamline communication, security, and routing. This pattern is essential when managing the complexities of microservice-based
applications.

														-------Gateway Routing pattern----------
The Gateway Routing pattern is a design pattern used in microservices architectures where an API Gateway routes incoming client
requests to the appropriate backend microservices based on various factors like the URL, headers, or request parameters.

															-------Gateway offloading Pattern-------
The Gateway Offloading Pattern is an architectural pattern used in microservices to offload certain cross-cutting concerns-such
as security, caching, rate limiting, and monitoring-from individual microservices to the API Gateway. This pattern helps centralize
and simplify the implementation of these concerns, allowing the microservices to focus solely on business logic.

																----Backend for Frontend (BFF) Pattern---------
The Backend for Frontend (BFF) Pattern is a design pattern used in microservices architectures where a separate backend service
is created for each client type (e.g., web, mobile, tablet). Each frontend (client) has its own specialized backend to optimize
communication between the frontend and the microservices, providing a tailored experience for different clients. THese are generlaly req like in case of web we need 
more data to display but in case of mobile we need compresss data and so, out device can run s/w easily i.e reasons we have seprate Gateway service is there for each cleint.

												-----------Gateway Aggregator/Composition pattern eazy bytes----------
In microservices architecture, a Gateway Aggregator or Gateway Composition pattern is used when a request from a client needs
to retrieve or process data from multiple backend microservices. Instead of having the client make multiple calls to various
microservices, the API Gateway consolidates the requests into a single response.(all applicable api will be called and combine the response and send it client, from
client side single api will be called).

														-----------RESILIENCY IN MICROSERVICES----------
HOW DO WE AVOID CASCADING FAILURES?
HOW DO WE HANDLE FAILURES GRACEFULLY WITH FALLBACKS?
HOW TO MAKE OUR SERVICES SELFHEALING CAPABLE

Solution:-									-----RESILIENCY USING RESILIENCE4J-----------

Resilience4j is a lightweight fault tolerance library designed for functional programming. It offers the following patterns for
increasing fault tolerance due to network problems or failure of any of the multiple services:
Circuit breaker - Used to stop making requests when a service invoked is
failing

Fallback - Alternative paths to failing requests
Retry - Used to make retries when a service has temporarily failed
Rate limit - Limits the number of calls that a service receives in a time
Bulkhead - Limits the number of outgoing concurrent requests to a service to avoid

Use case:-When a microservice responds slowly or fails to function, it can lead to the depletion of resource threads on the Edge server and intermediate services.
This, in turn, has a negative impact on the overall performance of the microservice network overloading.

												----CIRCUIT BREAKER PATTERN------
CIRCUIT BREAKER PATTERN
In Resilience4j, the circuit breaker is implemented via three states

This can be implmented at gateway level as well as Service layer of MS,on service leayer it work for single ms api.

CLOSED - Initially the circuit breaker starts with closed status and accepts client requests
OPEN - If Circuit breaker sees threshold requests are failing, then it will OPEN the circuit which will make requests fail fast and route to fallback method.
HALF_OPEN - Periodically Circuit breaker checks if the issue is resolved by allowing few requests. 
Based on the results it will either go to CLOSED or OPEN, we set threshold value for pass percentage, if it meets then circut will
be closed.
s
												----Circuit Breaker at gateway service---

Dependencies:-
spring-cloud-starter-circuitbreaker-reactor-resilience4j

we will add the circuit breaker filter where we have defined the cutom route:-

return routeLocatorBuilder.routes()
						.route(p -> p
								.path("/eazybank/accounts/**")
								.filters( f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)","/${segment}")
										.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())   //till here same code of routes.
										.circuitBreaker(config -> config.setName("accountsCircuitBreaker")  //Enabling circuit breaker filter
												.setFallbackUri("forward:/contactSupport"))) //fallback method
								.uri("lb://ACCOUNTS"))

YML configuration to tune circuit breaker:-
resilience4j.circuitbreaker:
  configs:
    default:						//this is default config , applicable to all services
      slidingWindowSize: 10			//This circuit will be applied by observing 10 req.
      permittedNumberOfCallsInHalfOpenState: 2 	//two req will be observed for coming to hallf open state
      failureRateThreshold: 50					//decision will taken on this percentage passing
      waitDurationInOpenState: 10000			//cooling/healing time 

=>TO domenstrate this pattern, we can hit the actuator circuitbraker endpoint , which will give us overall status of our circuit, if we hit the circuit breakerevent enpoint
which we call by the name of circuitbreaker it will give us list of all api call and its status, when we overwhelm the api with threshold values, we can see the 
status of the circuit changes and after cooling time it will make it half open and if succesful , it will go the close state.

FallBack method:-THis is the method call to give the signal to end user about the status of the circuit.insteied of error, we will get the customize error message.

=>circuit breaker fallback methods return Mono or Flux to maintain the non-blocking, asynchronous nature of the application's data flow. 
=>By returning a Mono or Flux, the fallback mechanism immediately provides an alternative data signal (e.g., an empty Mono, a default value wrapped in a Mono.just(),
or an error signal) without blocking the calling thread
