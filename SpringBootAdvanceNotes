
https://github.com/eazybytes/microservices
PDF need for revision
quesiton:
ENtity vs DTO.
DTO-https://martinfowler.com/eaaCatalog/dataTransferObject.html
Auditing and transacitons in spring data jpa:-
https://medium.com/@lavishj77/spring-data-jpa-auditing-and-transaction-management-b9e3246c2968
Documentation of Rest api:-we need add dependency of springdoc-openapi and it will add the swagger 
document of our apis through swagger endpoint and we can add more details using annotation from this dependencies.
                            -:Strangler pattern for microservice migration:-
https://www.geeksforgeeks.org/system-design/strangler-pattern-in-micro-services-system-design/

                                                          ----Docker ------
when we are using cloud services, cloud service providers have their physical infar at their data centers called servers but those are huge infra so, they can
not be given to single org .so, with help of Hypervisior concept they divide the servers resouces like ram, memory etc viutually into different parts called virtual
Matchine(VM), these vms are work as computer having memomory,os etc, so, we can use these vms according to our need of our application .
->Suppose we want to run our one of the microservices to the vm so, we need to install java version needed, data base and other dependencies so,we can run our serivce but
there is one issue with it,its not sure that our service will need all the resouces of the vm , in that case it will do the over price and for n number of services
we need to have same number of vm which is not a good practice. neither we can deploye all our services to same vm , that might cause resouces issue, dependencies issue
like java version,diff db etc.similerly scaling will be required manual intervention so, this way vm are not right for our microservices.

  VM
vm(os,libranry etc)
Hypervisior
Servers

solution of this problem is Container->its name suggest it contains all the required dependencies for particular service(simile ship container contains all needed resouces
in it for like aaple container will have ac in it)

COntainers
container1(lib,dependecies)
container engine(Docker)
Host OS
Servers

=>Even os is not there in container so,its light weight  and its super quick to restart  and destroy.
->WE will create image of our container which will be representation of our container, we can create any no of container from image like any no of obj we can create from
java class.
->Docker container is running repsentation of running image.

Follow notes from -file:///C:/Users/abhijeetkumar5/Desktop/LearningNotes-main/Learning/java%20interview%20questions/Advance%20spring%20boot/Master+Microservices+with+SpringBoot,Docker,Kubernetes.pdf


                                                                      Redis
download and unzip and start server, it will ready for accpeting traffic.
:-https://github.com/microsoftarchive/redis/releases/tag/win-3.2.100
Redis (which stands for REmote DIctionary Server) is an open-source, in-memory data structure store that is used as a database, cache, and message broker. 
Because all data resides in main memory (RAM), it delivers extremely high performance with sub-millisecond latency. 
=>https://github.com/Java-Techie-jt/spring-data-redis/tree/main =>Redis as in memory db
                                                      Redis for caching
https://github.com/Java-Techie-jt/spring-data-redis-cache
when  @GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product") added at the method of serive or controller layer, first search take the data from db but next it took it from cache,
we find diff in time of response in the postman along with log of calling db is not there when we hit sencond timee.
 @Cacheable(key = "#id",value = "product") //Id is the key of cache and value is the hash of cachedb.

=>@GetMapping("/{id}")
    @Cacheable(key = "#id",value = "product",unless = "#result.price>1000") //here product will be cache based on condition if price is less than 1000 only.
=>When we are deleting the data from db, we will have remove that data from cache too, here we do so:-

@DeleteMapping("/{id}")
    @CacheEvict(key="#id",value = "product")// key and value would same as we using in cacheable. it will remove the data from cache.
 @CachePut(value = "users", key = "#user.id")=> THis will update the the data to cache when we add new data to user.

-----------------------------------------------------------------------Creating image of our application using docker-------------------------------------
First we have to mention the packing of our application in pom.xml file at place where name and verion of applicaiton is mentioned.
===>> <packaging>jar</packaging>
=>Before we make code changes for docker, when we build the application, we will getting the jar craeted for our app, which will have all code and lib but not java runtime
this is also called fat jar having big sizw e.g- accounts-0.0.1-SNAPSHOT.jar
This is created with ref of name from pom:-
<artifactId>accounts</artifactId>
<version>0.0.1-SNAPSHOT</version>
=>mvn spring-boot:run ---->Runnnig from cmd.
=>java -jar target/accounts-0.0.1-SNAPSHOT.jar ------------------->run spring boot using java
--------------------------------------------------------Creating Docker file --------------------
#Start with a base image containing Java runtime:tag like version of jdk image.Java is need for our application to run so, we are using java image which is there at dockerhub.
FROM openjdk:21-jdk-slim

# MAINTAINER instruction is deprecated in favor of using label
# MAINTAINER eazybytes.com
#Information around who maintains the image
LABEL "org.opencontainers.image.authors"="eazybytes.com"

# Add the application's jar to the image
//here we are copying our jar file from target folder which is generated post build to image(which has java and now our applicaiton is there with)

COPY target/accounts-0.0.1-SNAPSHOT.jar accounts-0.0.1-SNAPSHOT.jar

# execute the application, this is the same cmd which we used to run our app using java cmd , there was space in bwn but here commona
ENTRYPOINT ["java", "-jar", "accounts-0.0.1-SNAPSHOT.jar"]


--------------------------------------------------------------------Running docke image-------------------------------------------
First check if docker is running in our system, cmd:- docker version
=>Build docker image: ->docker build . -t easybyte/accounts:s4
format:-docker build path -t(tag) userId/nameOfapp:tag(version)
->When we build it from cmd, first it will download the openjdk from dockerhub and try to make the image by copying our jar file and jdk to it.
------------------------------------------------Running docker containere---------------
docker run -p 8080:8080 ak608333319/accounts:s4 :--p(port fisrt port is for our local system, second port is for docker to expose at 8080 , since docker run on 
isolated network, we can run same image on same port multiple at same time since, they are isolated so, every private network can run on same)
docker run -d -p 8080:8080 ak608333319/accounts:s4 //-d means runnig in detach mode so, other cmd can be fired form cmd while one is running
docker run -d -p 8081:8080 ak608333319/accounts:s4

Above two containers are runnig same image but on diff port  , so we have scaled the our app to two pod here.
We are using same port number for docker to expose for two diff pod is mained by docker call Port mapping .

`````````````````````````````````````````````````````````Buildpacks--------------
Buildpacks are a technology that automates the process of turning application source code into runnable container images, typically without the need for a Dockerfile.
They achieve this by automatically detecting the programming language and framework of an application, installing necessary dependencies,
and configuring the runtime environment.

we will add jar packing to the pom file:-
<packaging>jar</packaging>

we will add the image tag to the plugin section of the pom file:-

                 <configuration>
                    <image>
                        <name>eazybytes/${project.artifactId}:s4</name>
                    </image>
                </configuration>

=>Run the build cmd which will scan the whole project and build the image using all dependencies and we do not have to write the insturction to include
java veresion copy the  fat jar from tartet to our image etc.
=>This is required to run the docker in background.
=>mvn spring-boot:build-image  //Generating docker image form mvn.
==>RUnning the new image generated:-
docker images =>get the list of image
=>docker run -d -p 8090:9090 eazybytes/loans:s4 =>run the image with its name.
we will able to see image is running  on docker , we can hit the api from postman to validate too.

-------------------------------------------------------------------------Pushing docker image to DockerHub-----------

docker images
docker image push docker.io/ak608333319/accounts:s4 

//this will push the image like same we does in github, here we have already login from our account in docker desktop
so, cread will be taken from there and our image will be pushed to hub without explicity asked for creds.

we can see this in reps:-https://hub.docker.com/repository/docker/ak608333319/accounts/general //same image can be used in pipeline to deployed in cloud.
=>First delete the accounts container and image then we will try to pull the image from hub and run it.

Anyone want to use this they can pull it:-docker pull ak608333319/accounts:s4
=>ocker run -d -p 8080:8080 ak608333319/accounts:s4 //running the image pulled from dockerhub
                                                    ----docker compose--------
Docker Compose is a tool for defining and running multi-container Docker applications. It simplifies the orchestration of multiple services that work together to form a
complete application, such as a web server, a database, and a caching layer. 
Docker Compose uses a single YAML file (typically compose.yml or docker-compose.yml) to define all the services, networks, and volumes required for your application.
This centralizes the configuration and makes it easy to manage.
sample file:-we will maintain the herarchie of yml

services:                                //this is the root level where list of services will be mentioned 
  accounts:                              //Microservice level infos
    image: "eazybytes/accounts:s4"       //image of account serice
    container_name: accounts-ms          //name of container else random will be assigned as we saw in desktop(docker)
    ports:                               //port at which docker and our sevices will listen
      - "8080:8080"
    deploy:                              //here we gave limitation to our app which uses the resources of docker
      resources:
        limits:
          memory: 700m                   //max 700 mb will be used by account services
    networks:                            //as we know docker run in isolated network so, if our services need to communicated with each other we need to have common network
      - eazybank
  loans:
    image: "eazybytes/loans:s4"
    container_name: loans-ms
    ports:
      - "8090:8090"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
  cards:
    image: "eazybytes/cards:s4"
    container_name: cards-ms
    ports:
      - "9000:9000"
    deploy:
      resources:
        limits:
          memory: 700m
    networks:
      - eazybank
networks:                    //here we are defining the common network where all our apps will be running, network name is easybank and bridge will be used as driver to communicate
  eazybank:
    driver: "bridge"


We can run the compose file with cmd so, all of the services image will start running together, we do not have to do it each and every service wise neither we
have to create docker file for each service.

CMD:->docker compose up -d {running in detach mode, it will download the all dependencies and start our image , we need to hit the cmd from the location where our compose file}
      //created containers
CMD->docker copose down -d {stopped and revmove the running image}, it will be removed we can check by doing docker ps.
CMD->docker compose start (start the stopped containers only, not removed containers)
CMD->docker compose stop (stop the the running container, if not running container it will give error message)


                                            ----Imp cmd of Docker--------
Docker Commands used in the course                       Docker Command	Description

"docker build . -t eazybytes/accounts:s4"   	            To generate a docker image based on a Dockerfile
"docker run -p 8080:8080 eazybytes/accounts:s4"	          To start a docker container based on a given image
"docker images"                                          	To list all the docker images present in the Docker server
"docker image inspect image-id"	                          To display detailed image information for a given image id
"docker image rm image-id"	                              To remove one or more images for a given image ids
"docker image push docker.io/eazybytes/accounts:s4"	      To push an image or a repository to a registry
"docker image pull docker.io/eazybytes/accounts:s4"	      To pull an image or a repository from a registry
"docker ps"		                                            To show all running containers
"docker ps -a"		                                        To show all containers including running and stopped
"docker container start container-id"		                  To start one or more stopped containers
"docker container pause container-id"	                  	To pause all processes within one or more containers
"docker container unpause container-id"		                To unpause all processes within one or more containers
"docker container stop container-id"		                  To stop one or more running containers
"docker container kill container-id"		                  To kill one or more running containers instantly
"docker container restart container-id"	                  To restart one or more containers
"docker container inspect container-id"		                  To inspect all the details for a given container id
"docker container logs container-id"		                  To fetch the logs of a given container id
"docker container logs -f container-id"		                  To follow log output of a given container id
"docker container rm container-id"		                  To remove one or more containers based on container ids
"docker container prune"		                              To remove all stopped containers
"docker compose up"		                 	                   Creates and starts containers based on the given Docker Compose file
"docker compose down"		                 	                   Stops and removes containers, networks, volumes, and images created by up
"docker compose start"		                                Starts existing (previously created) containers without recreating them
"docker compose stop"		                                  Stops running containers without removing them
"docker run -p 3306:3306 --name accountsdb -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=accountsdb -d mysql"        	        To create a MySQL DB container
"docker run -p 6379:6379 --name eazyredis -d redis"        	To create a Redis Container
"docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:22.0.3 start-dev"	To create Keycloak Container



                                                                            DOcker extention
Docker Extensions are add-ons that integrate third-party tools and functionalities directly into Docker Desktop, enhancing its capabilities. 
They allow users to seamlessly connect their preferred development tools to their application development and deployment workflows. 
 section 4 code:-https://github.com/eazybytes/microservices/tree/3.4.1/section4

Cloud native application:-
A cloud-native application is a software application designed and built specifically to run in a cloud environment, taking full advantage of its scalability,
flexibility, and resilience. These applications are built using a microservices architecture, where functionality is broken down into small, independent services.
Key technologies include containerization (like Docker and Kubernetes), CI/CD pipelines, and serverless functions. 
cloud native app can be on gcp,aws,azure etc.
Key characteristics

Agile development: They support agile development practices, which allows for rapid deployment of new features and faster time-to-market. 
Scalability and resilience
Automation:ci/cd
Containerization:docker or othe tool
Microservices architecture:

                                                =======12 factos and 15 factor Methodology============
The "15-Factor Methodology" is a set of principles for developing modern, cloud-native applications, an extension of the original "Twelve-Factor App" methodology. 
The 15 factors are:
1.One Codebase, one Application : Each application has only one code base which will be independently deployeing using packing (build once) and run in multiple env
we should have config of diff env externally injected during deployement, so, once its build will be running same in all env.Its easy to maintain and tack.

2.API First: Applications should be designed with an API-first approach, defining service contracts upfront using a language like Open API Specification (Swagger).
dictates that applications should be designed with their Application Programming Interfaces (APIs) as "first-class citizens" from the outset of the development process.
This means the API contract and design are defined and agreed upon before writing any application code or building user interfaces. 

3.Dependency Management: Explicitly declare and isolate dependencies using a dependency declaration manifest and an isolation tool.we use maven or gradle where we declr
our dependencies which will be download and mangaed by it, we don't need to download explicitely.

4.Design, Build, Release, Run: Strictly separate the build, release, and run stages to create an immutable build artifact at each release.All setsps shold be followed and 
does not  make any changes at any stage , so make it immutable. 

5.Config: Store configuration that varies between environments (staging, production, etc.) in environment variables or external configuration services, never in the code.

6.Logs: Treat logs as event streams, directing them to standard output and leaving the aggregation and storage to external tools.so, we can see the logs of all the MS at single place.

7.Disposability: Maximize robustness by designing processes that can be started or stopped at a moment's notice with fast startup and graceful shutdown.In the cloud native
we have docker which make our application to light image which will be easy to deploye and stopped if needed , while stopping our aaplicatin should not take more req and 
complete the ongoing req, in case of neeed it will upscale and downsacle autometically, if somthing went wrong it will be restart the app, these capablity is acheive by
kubernative, so,

8.Backing Services: Treat all backing services (databases, message brokers, caching systems, etc.), whether local or third-party, as attached resources accessed via a URL/credentials stored in the config.

9.Environment Parity: Keep development, staging, and production environments as similar as possible to minimize gaps in time, people, and tools.

10.Administrative Processes: Run admin/management tasks (database migrations, batch jobs, etc.) as one-off processes in an identical environment to the regular long-running app processes.
Thease should be treated as isolated process and deplouyed in each env.we should deployed thease as seperate MS.

11.Port Binding: Applications should be self-contained and expose their services via port binding, not relying on runtime injection of a web server.

12.Stateless Processes: Execute the app as one or more stateless and share-nothing processes, storing any necessary persistent data in a stateful backing service.
should not store any data in session or any where expect db,it won't have same instance in next req, so, all instance are isolated to each other.

13.Concurrency: Scale out via the process model, designing applications to distribute workload across multiple processes.
In java app JVM handle this multi threading by using thread from thread pool, by default we have 200 thread avail to serve the client.

14.Telemetry: Design the application to include the collection of monitoring data, health information, and statistics (APM, domain-specific data, system logs).

25.Authentication & Authorization: Implement robust security measures, especially for APIs, using solutions like OAuth2 or OpenID Connect to handle user authentication and authorization. 

                                      ==========Configuration Management ============
If we bind all code and config to the application, we need to build the image in every other env since, our config is hard binded to code, so this process would be not be
good practice, so, config should be externally managed and once our code image is build can be used in all env , out config would be inject at the time of deployment
it would be env wise so, we will be free from overhead of config.

=>Spring boot let us externalise the configuration so, we can use the same code base in all env .
=>Bydefault Spring boot look for config in application.properties, it has set of priority which will override the configs;-
application .properties<os Env veriable<java system properties(System.getProperties())<JIndi components<Servelet init paramenters<servelet config parameters<command line arg.

=>we can read the config injava using 
1.@value , it will read one value at time
@Value("${build.version}")
    private String buildVersion;

2.Environement interface->env.getProperties() one config at time, this is used to read the env veriable like secret,password;
@Autowired
private Environment environment;
 @GetMapping("/java-version")
    public ResponseEntity<String> getJavaVersion() {
        return ResponseEntity
                .status(HttpStatus.OK)
                .body(environment.getProperty("JAVA_HOME"));//this will return the path of our java insalled in system.
    }

3.@ConfigurationProperties("prefix")->we can define the pojo and it will read the all config with specified prefix to a pojo.

confog from yml:-
accounts:
  message: "Welcome to EazyBank accounts related local APIs "
  contactDetails:
    name: "John Doe - Developer"
    email: "john@eazybank.com"
  onCallSupport:
    - (555) 555-1234
    - (555) 523-1345

Reading into pojo of Record class:-
@ConfigurationProperties(prefix = "accounts")
public record AccountsContactInfoDto(String message, Map<String, String> contactDetails, List<String> onCallSupport) {
//THe dto will be populated during starting of app.
}
@EnableConfigurationProperties(value = {AccountsContactInfoDto.class}) we need to add this to our application class.

