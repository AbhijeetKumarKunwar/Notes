Azure Cosmos DB is a fully managed, globally distributed, multi-model database service offered by Microsoft Azure.
It is designed to provide high availability, scalability, and low-latency access to data for modern applications.
Key features of Azure Cosmos DB include:

Global Distribution:
It allows data distribution across multiple Azure regions, ensuring low-latency access for users worldwide and high availability.

Multi-Model Support:
Azure Cosmos DB natively supports various data models, including document (NoSQL), key-value, graph (Gremlin), and column-family (Cassandra), providing flexibility for different application needs.

Automatic and Instant Scalability:
It automatically scales throughput and storage based on application demands, eliminating the need for manual capacity planning.

Guaranteed Performance:
It offers guaranteed single-digit millisecond latency and high availability with financially backed SLAs.

Multiple Consistency Models:
It provides a range of consistency models (eventual, consistent prefix, session, bounded-staleness, and strong) to balance consistency and performance based on application requirements.

Serverless and Provisioned Throughput Options:
It offers both serverless and provisioned throughput capacity modes, allowing optimization for cost and performance based on workload patterns.
API IN COSMOS DB:-

Azure Cosmos DB offers multiple APIs to interact with your data, providing flexibility based on your application's needs and existing skillsets. These APIs allow you to leverage Azure Cosmos DB's global distribution, elastic scalability, and guaranteed low latency while using familiar data models and query languages.
The primary APIs available are:

NoSQL (Core) API:
This is the native API for Azure Cosmos DB, designed for document-oriented data. It supports SQL-like queries for JSON documents and is ideal for new applications or those requiring high performance and flexibility.

MongoDB API:
This API provides wire protocol compatibility with MongoDB, allowing you to use existing MongoDB drivers, SDKs, and tools with Azure Cosmos DB. It's suitable for migrating existing MongoDB applications with minimal code changes.

PostgreSQL API:
This API offers PostgreSQL compatibility, enabling you to use Azure Cosmos DB as a PostgreSQL database. It supports relational data models, SQL queries, and features like JSON support and geospatial capabilities.

Cassandra API:
This API provides compatibility with Apache Cassandra, allowing you to use Cassandra drivers and tools with Azure Cosmos DB. It's suitable for migrating existing Cassandra-based applications.

Gremlin API:
This API is designed for graph data models, allowing you to store and query data as vertices and edges using the Gremlin traversal language. It's ideal for applications requiring graph-based analysis, such as social networks or recommendation engines.

Table API:
This API provides compatibility with Azure Table Storage, offering a key-value store for high-throughput, low-latency applications. It's suitable for migrating existing Azure Table Storage applications.
=>we have to create acount for each api, one account will not work for other account api.

Why No Sql DB?
->High volume of data
->Dynamic schema {data from socia media}
->Horizental scaling
->don't force schema

DATA MODELS in NO SQL:-
There are four primary types of NoSQL data models:

Document Databases:
These store data in flexible, semi-structured documents, often in formats like JSON or BSON. Each document can have a different structure, making them well-suited for handling diverse and evolving data. Examples include MongoDB and Apache CouchDB.,cosmos db.

Key-Value Stores:
This is the simplest NoSQL model, where data is stored as a collection of key-value pairs. Each key is unique and maps to a specific value, which can be any type of data. Examples include Redis and Amazon DynamoDB.azure table.

Wide-Column Stores (Column-Family Databases):
These organize data into tables, but unlike relational databases, columns can be added dynamically, and rows can have different sets of columns. This model is optimized for handling large datasets with high read and write throughput. Examples include Apache Cassandra and HBase.

Graph Databases:
These are designed to store and query highly connected data, representing entities as "nodes" and their relationships as "edges." This model is ideal for applications involving complex relationships, such as social networks or recommendation engines. Examples include Neo4j and OrientDB.,apache gemlin

NoSQL data models are chosen for their flexibility, scalability (especially horizontal scalability), and performance in handling large volumes of diverse data, making them suitable for modern applications like big data analytics, real-time applications, and content management systems.

Why to use nosql db with document  data model ?
->json is commonly used data format
-> cosmos db supports json cosmos natively
->no need to predefine schema
->Diff type of data can be stored together.

Advantages it provides:-
Guaranteed speeed
fully managed serverless.(users do not need to provision or manage the underlying infrastructure or manually configure throughput capacity)

=>When to go for Nosql cosmos db:-
Unpredictable spike and high dip in traffic.
Applicaiton generate lots of data
Business continuty

----Component of cosmos db for no sql:--
1.Account:- A Database Account is the top-level resource, providing a unique endpoint for accessing Cosmos DB resources A Database     
Account is the top-level resource, providing a unique endpoint for accessing Cosmos DB resources.
2.Database:- Databases act as namespaces, managing users and permissions.
3.COntainer:-Containers, similar to tables, store items, which can be documents, rows, or other atomic data structures, depending on the chosen AP
4.items:-data


======================================Creating azure cosmos db account for nosql======
https://portal.azure.com/#home
#########Basic####
->database->azure cosmos db->subscription tye
->Resouce group (logical group where all db will be there)
->Account Name->unique account name
->location where our appliaciton is deployed
->capacity mode->provisioned thoughput(rented resouces dedicatedly) /serverless(pay as you go autometically)
->Provisioned throughput->in case of traffic is highly predictable
->checked for limited provisioned thorught put is there to controll the pricing.

###########Global distribution########
radio button
=>Geo Redendency->DAta would replicated in other resons as Read-only mode to increase the availablity e.g as we selected the East us in region under basic, if we enabled this then it will be paired to us West too in read only mode.
=>Multi Region write:->if we enabled this data would be availble in paired region with write access,( can be written to all region)
=>Availablity zone:->Data would be replicated to all zone of selected region.

################ Network###############
we can select the all network, public , private according to ned.

##############Backup Policy #############
radio button
->periodic->we need to give the period time->backup interval (in how many hour of interval we need to take backup),->no of days
suppose it is taking backup interval of 6 hour so, it will create copy of 4 in 24 hour , if we does for 25 days , it will create
copy of 60 files.
->conineus 7 days ->free
->continues 30 days {cost} can be retrive any time  within the window.

#################### Encyption################
encyption type->service managed key OR custom managed key {encryption of data}
####################Tag##############
Key value label e.d Env :learning
-------------------account will be create the reviewing the details--------------
post creation we will get the url to access the DB.
RU=>
Request Units (RUs) are a performance currency of system resources (such as CPU, IOPS, and memory)
=>we have diff option to explore like activity log to see all log of activity,iam, etc


-----------------------------------Creating database---------
creating container is same as creating tables
when we will go to new , we will have option of new container/new Database
->new Data base->ask for databaseId(name)->if we check at provision thoughPut->we have to give the thorugput option ->
autoscale(we need to give the RU/s , first it will use 10 percent of limit and increase in the case of trafic) /Manual-> RU/s->OK to create db.
->when we create db without checking Provisioned Throughput in that case we don't need to give RU/s to it , it will use the autoscaling i.e serverless which will allocate the RU accourding to the consumption and will be charged accordingly.
->if we are creating other db with Provisioned Throughtput of x*1000 , it should not excced the limit we provisioned on account level, if we have given 1000 at account level we can max give 1000 during db creation, if we going beyond that we
need to increase it form billing section.

----------------------------------creating container (Table)------------
click on three dot of database, it will give option to create new container->we can select the data id which we have created
->containerId will be table name->partition key is like primery key in RDBMS it will be used to improve search 
->provisioned dedicated throughput for this table will give the dedicated RUs to this container otherwise, if we not select
this , what ever RU has been given to the Database level will be utilize by all container of the database
->OK will create the table inside the container.

---------------------------Adding item to container-------------
click on item of database and do new item ->text box will be appear ->place json->save->post save we will get additional
feild in the jason including id filed which is being generated by cosmos and it will be unique value and in the list of 
item preview this id will be shown along with partition key given, if we give the id explicitly in our json, it will not 
generate id , ours will be shown there.
->we can query the data based on any filed of json e.g-
=>select * from c where c.customer.firstName='sam' //we can access the filed like normal jason access using . operator.

----------------------------RU---------------
Each api has its own set of db operation it points form simple read query to complext db operaiton , in each case it used
DB system resounces based on complexity of operation
RU Request Unit->system resouces are expressed in term of RU , which is combination of cpu, memory and IOPS, or Input/Output Operations Per Second.
1 RU->cost to read point 1 kb item ,regardless of which api of cosmos u are using, its common for all as unit.
we configure this while creatin account and also at time of creating db.Par

--------------------------------ThroughPut-----------
=>number if req can Db handle per second is thorughput , its unit is Ru/s
=>we can define the TP at database layer(it will share over all the container of the db),container layer(it will use the allocated RU/s at each container layer)
or both(IN the case of mixed i,e data base is define with set of RU/s will be shared over the all the container which are not define with RU/s at container layer , amout
would be depend on the workload , it will be totalAllocatedAt container -dedicated allocated to the container,it means remaining RU will only shared ), 
e.g more TP means we need to define more Ru to our container.

---------------------Cost management---------
We can calculate the price using pricing calculator, it depend upon the worload RU/s used ,reagions etc.
-------------------------------Horizental scale-------------------
Each container has limit of saving data once, its reach next new container is created to store the data with same machine config, so it will scale its self .

-------------------------------how search work---------
->query recive by sdk ->will check the partition key with hash method->corresponding logical partition will be found against it ->routing table has logical and physical
mapping so, it will go to the physical partition where actual data is stored->there it will do the local index search and fianlly it will go the logical partition 
and get the data against the key.

--------------------------------Partition and partition key--------------
->we can not create container without PK, it can possible with legacy sdk.
It is used the divide the data in such a way that its improve the query performace to retrive the set of data frequently.it is being done on creteria which is partition key.
->The items in a container are divided into distinct subsets called logical partitions. 
Logical partitions are formed based on the value of a partition key that is associated with each item in a container
->suppose we have city is partition key then we will have logical parition if each city and if we are getting data related to deli , it will go to that partion only , if we 
value is getting added, it will create new partition.
. All the items in a logical partition have the same partition key value.
->When you select a partition key, Cosmos DB uses a hash of that key's value to determine the logical partition for each item. 
These logical partitions are then placed onto the physical partitions. 
->Each logical partition is linked with physical partion through hasing and its managed by azure, programmer can only manage the logical partition.
explain:-https://learn.microsoft.com/en-us/azure/cosmos-db/partitioning-overview
------------------------------------------------
Why a partition key is required
->The partition key is essential for Cosmos DB to function as a distributed, scalable database.
->Enables horizontal scaling: It allows Cosmos DB to distribute your data and throughput across multiple physical partitions. Without a partition key, 
the database would not be able to scale beyond the limits of a single server.
->Optimizes queries: When you provide the partition key in a query, Cosmos DB can route the request directly to the specific physical partition containing the data,
avoiding a performance-intensive cross-partition query.
Ensures consistency and transactions: All transactions within a stored procedure or trigger are scoped to a single logical partition. The partition key defines this scope,
allowing for multi-item ACID transactions. 

-
Logical partitioning and data distribution:-
=>Item ingestion and hashing: When you create or update an item in Cosmos DB, the system uses a hash algorithm on the item's partition key value.
=>Logical partition assignment: The hash of the partition key determines which logical partition the item belongs to. All items with the same partition key value
reside in the same logical partition.
=Logical-to-physical mapping: Cosmos DB automatically maps these logical partitions to physical partitions. Many logical partitions can be mapped 
to a single physical partition, but a single logical partition will never span multiple physical partitions.
=>Scaling and load balancing: As a container's storage or throughput increases, Cosmos DB automatically and transparently moves logical partitions 
to new physical partitions to balance the load. This horizontal scaling ensures the system can handle large and growing workloads. 



----------------------------------How it make search faster-------------
->It's a common misconception that Cosmos DB does a full scan of the partition key hash space. Instead, it uses hashing and routing tables 
to directly go to the correct physical partition, which is what makes it faster and more scalable than a full table scan in a relational database. 
->Request from SDK: When you issue a query with a partition key filter (e.g., SELECT * FROM c WHERE c.userId = "user123"), the Cosmos DB SDK on your client first parses it.
->No "full scan": The system does not do a full scan of all possible partition key hashes. Instead, it takes the value "user123", runs it through a hash function, 
and immediately determines the exact logical partition where that data is stored.
->Direct routing: The Cosmos DB gateway or SDK's routing table (depending on the connectivity mode) already knows the mapping of logical partition ranges to 
physical partitions and their network addresses. This allows the request to be routed directly to the specific physical partition containing the logical partition for "user123", bypassing all other partitions.
Local index lookup: Once the request arrives at the correct physical partition, it performs a search on that partition's independent, local index.
Cosmos DB automatically indexes all properties by default, so it can perform an efficient index lookup to find all items for "user123".
The scale of this index is much smaller than a full table index, which makes the lookup very fast. 
