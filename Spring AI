https://github.com/eazybytes/spring-ai.git

section1 controller will intract with open api llm using chatClient using chatClient.prompt(message).call().content();
method, we will use spring.ai.openai.api-key=<your-openai-api-key> 
in our properties file to malk connection with open api, there we will have tokens which will be used to intract with llm.

`````````````````````````What is ChatModel?
ChatModel is the lower-level abstraction that represents the actual Al model interface. It's the core component that:
=>Defines the contract for communicating with different Al providers (OpenAI, Azure OpenAI, Anthropic, etc.)
=>Handles the actual API calls to the Al services
=>Manages model-specific configurations and parameters
=>Provides the foundational layer for Al interactions
Examples of implementations:
OpenAiChatModel (for OpenAI)
GeminiChatModel (for Google)
MistralChatModel, etc.
\
==````````````````````What is ChatClient?
ChatClient is a higher-level, more developer-friendly abstraction built on top of ChatModel. It provides:
A fluent API for easier interaction with Al models
Better developer experience with method chaining
Simplified prompt construction and message handling
Supports both synchronous and streaming programming models
ChatClient is the friendly wrapper (or service layer) around a ChatModel.
It takes care of:
Building a Prompt
Managing chat history
Invoking the model
Extracting the content


----------------------`````````````````````````Relation of ChatModel and chatClient---------
ChatClient uses ChatModel internally but wraps it with a more
convenient API. When you use ChatClient, it eventually delegates
to the underlying ChatModel to make the actual Al service calls.
-----------------------------------------------how they work together-------------------

How They Work Together
-Spring Boot Autoconfiguration: When you add Spring Al dependencies, Spring Boot automatically creates ChatModel beans for
configured Al providers
-ChatClient.Builder Creation: The framework provides an autoconfigured ChatClient.Builder that's already wired with the
appropriate ChatModel
-Fluent API Usage: You use ChatClient's fluent methods to build prompts, which internally get converted to the format
expected by the ChatModel
-Execution: ChatClient delegates to ChatModel to send requests to the Al service and handle responses
Example Flow
Your Code t ChatClient (fluent API) → ChatModel (AI service integration) → AI Provider API
This design follows the common pattern of having a low-level technical interface (ChatModel) and a high-level user-friendly
interface (ChatClient) that makes the framework more accessible while maintaining flexibility for advanced use cases.

                    ------------running ollama----------
ollama run llama3
config:-
spring.ai.model.chat=ollama
spring.ai.ollama.chat.options.model=llama3.
same code will work for this alos, since chatModel bean will take care of it and chatClient will intract with ollama
restApi running on our local http://localhost:11434/, its llm will processed our request on our local matchine, its downlaod
the model during the time of installation and does not need any internet, larger context version we will install we will
get more boundry of data.

---------------------------------------------------------
similerly we can do for Spring Al & Docker and AWS Bedrock.

------------------------Working with Multiple Chat Models in Spring Al---------------
when we does this we don't rely on chatModel to cretae bean ,since we have multiple model in taht case we explicitly create
bean of each of model using config and we used accroding to use and it might be used in the cse of fault tolarance.
->pom will have both dependecny

@Configuration
public class ChatClientConfig{
//frame work will inject the dependencies during the application boot.
@Bean
public ChatClient openAiChatClient(OpenAiChatModel chatModel) {
      return ChatClient.create(chatModel);
@Bean
public ChatClient ollamaChatClient (OllamaChatModel chatModel)
  ChatClient.Builder chatClientBulder =ChatClient.builder(ollamaChatModel);
    return chatClientBulder.build();
}

-------------------------------------------------------Message role--------------------------------
Here’s how a typical message sequence might look (in JSON-like format):
It provides a crucial structural component that helps the model understand the context, intent, and proper flow of dialogue.
JSON[
  {"role": "system", "content": "You are a helpful assistant."},
  {"role": "user", "content": "What is the capital of France?"},
  {"role": "assistant", "content": "The capital of France is Paris."},
  {"role": "user", "content": "Tell me a fun fact about Paris."}
]


The system message sets the context.
The user messages are questions or prompts.
The assistant messages are the model’s replies.

--------------------------------This is the code used to intract with the llm models------------
@GetMapping("/chat")
    public String chat(@RequestParam("message") String message) {
        return chatClient.prompt(message).call().content();
    }

when we tried to set the Message role, we will set the context and restrict our AI application to answer to specific set
of zoner . eg. we have set the system role for answering HR policy context and if we ask anything apart of this,we will
only get the answer related to HR and company policy only.
e.g-
  @GetMapping("/chat")
    public String chat(@RequestParam("message") String message) {
        return chatClient
                .prompt()
                //.advisors(new TokenUsageAuditAdvisor())
                .system("""
                        You are an internal IT helpdesk assistant. Your role is to assist 
                        employees with IT-related issues such as resetting passwords, 
                        unlocking accounts, and answering questions related to IT policies.
                        If a user requests help with anything outside of these 
                        responsibilities, respond politely and inform them that you are 
                        only able to assist with IT support tasks within your defined scope.
                        """)
                .user(message)
                .call().content();
    }
Questions:-http://localhost:8080/api/chat?message=what are the message role use by llm
Answer:-Nice to meet you! My name is Emma, and I'm an internal IT helpdesk assistant, 
trained on a specific set of predefined responses and guidelines. I don't have a human model or personal experiences, 
but I'm designed to assist employees with their IT-related queries in a helpful and efficient manner.

-----------------------------Understanding Message Roles in LLMs---------------
we uses default method when we want specific set of message used in every api call, in that case we feed our message during
client object creation,(it will work as aspect programming)
=>Creating the ChatClient with a Default System Message

this.chatClient = chatClientBuilder.defaultSystem("
You are an internal HR assistant...
""").build();

=>Sets the default behavior/personality of the Al assistant.
=>Used for general fallback behavior across requests.
=>Overriding System Message (Optional) in Runtime
chatClient.prompt()
.system ("""
You are an internal IT helpdesk assistant...
""")

=>
You want to avoid repeating the same system message or advisor every time.
You have a consistent role or tone for your assistant.

Overrides the default system message.
Useful for changing the assistant's role dynamically (e.g., HR instead of IT).

-------------------------Common default methods----------------------

defaultSystem(...) - Defines a default system message -
which sets the behavior or role of the Al assistant.

defaultAdvisors(...) - Registers advisors like logging or custom filters that
apply to each interaction.
chatClientBuilder.defaultAdvisors(new SimpleLoggerAdvisor() (

defaultTools(..)- Registers tools/functions available to the LLM by default.
chatClientBuilder.defaultTools(
List.of (myToolClassInstance))

defaultOptions(.) - Allows you to set default configuration options for
the model request (e.g., temperature, maxTokens, etc.)
chatClientBuilder
defaultOptions (ChatOptions.builder().
temperature(0.3).maxTokens(300).build())

defaultUser(.) - Sets a default user message that will be included in every
prompt unless overridden.

chatClientBuilder.defaultUser("How can you help me ?");

we can seperate the bean creation of chatClient and it will have system and userdefault promt, if we are not sending any
promt during call(), it will take that as input promt.

Config class to create Chatclient Bean:-
@Configuration
public class ChatConfig {

    @Bean
    public ChatClient chatClient(ChatClient.Builder builder) {
        return builder
                .defaultSystem("""
                        You are an internal IT helpdesk assistant. Your role is to assist 
                        employees with IT-related issues such as resetting passwords, 
                        unlocking accounts, and answering questions related to IT policies.
                        If a user requests help with anything outside of these 
                        responsibilities, respond politely and inform them that you are 
                        only able to assist with IT support tasks within your defined scope.
                        """)
                .defaultUser("how can i help you")
                .build();
    }

}
--------------------------------------------------------Using Prompt Templates in Spring Al-----------
Why Use Prompt Templates?

=>Simplifies prompt construction
=>Makes prompts reusable and maintainable
=>Keeps logic and text cleanly separated
=>Supports parameterized placeholders (like {customerName})

Where can Prompt Templates Live?
You can store them in:
src/main/resources/promptTemplates/userPromptTemplate.st
File extension: .st (StringTemplate)

e,g-
A customer named {customerName} sent the following message:
               "{customerMessage}"

Write a polite and helpful email response addressing the issue.
Maintain a professional tone and provide reassurance.

Respond as if you're writing the email body only. Don't include subject,
signature



How to Use in Code
@Value("classpath:/promptTemplates/userPromptTemplate.st")
Resource userPromptTemplate;

----------------------------------Using Prompt Templates in Spring Al--------------

chatClient
        .prompt()
                .system("""
                        You are a professional customer service assistant which helps drafting email
                        responses to improve the productivity of the customer support team
                        """)
                .user(promptTemplateSpec ->
                        promptTemplateSpec.text(userPromptTemplate)
                                .param("customerName", customerName)//dynamic values will be coming from req parm
                                .param("customerMessage", customerMessage))//dynamic values will be coming from req parm
                .call().content();

uri:->http://localhost:8080/api/email?customerName=Abhijeet&customerMessage=I am facing issue in my password reset

Response:-
Dear Abhijeet,

Thank you for reaching out to us about your password reset issue. We apologize that you're experiencing trouble with resetting your password, and we're here to help.

Could you please provide more details about what's happening when you try to reset your password? For example, are you receiving an error message, or is the process not completing successfully?

We'll do our best to troubleshoot the issue and get you back up and running as quickly as possible. In the meantime, if you have any further questions or concerns, please don't hesitate to let us know.

We're committed to providing excellent support, and we appreciate your patience and understanding as we work through this with you.

Best regards,


---------------------------------------------------Prompt Stuffing-----------------------------------------
Prompt Stuffing = Giving the LLM an open book before answering a question.
You include contextual data or reference text along with the user's question.
The LLM uses this extra content to answer the question accurately — even if it was not pre-trained on the topic.
This technique is also known as in-context learning or retrieval-augmented prompting (when done programmatically).
THis is limited to have info like of upto 100 lines, for more we uses RAG for trained our model.
e,g-----------------

Example: Internal company policy lookup
Prompt (Stuffed):
"According to the company's HR policy, employees are
eligible for 18 days of paid leave annually. Unused leave
can be carried over to the next year."
Question:
How many paid leaves do employees get each year?
LLM Answer:
Employees are eligible for 18 days of paid leave annually.

In the coming sections, we will learn how to apply a technique called Retrieval Augmented
Generation (RAG) to provide relevant context in prompts without exceeding the token limit.
------------------------------------------------
since, our model is stuff with this data:-https://github.com/eazybytes/spring-ai/blob/main/section02/springai/src/main/resources/promptTemplates/systemPromptTemplate.st
we will be getting answer by llm by computing this set of data.
e.g:-http://localhost:8080/api/prompt-stuffing?message=i have used 5 leave this year, how many leves would be there next year
REsponse:-
According to our HR Policy, employees are entitled to 18 days of paid leave annually. Since you've already used 5 leaves this year, that means you still have:

18 - 5 = 13

days of paid leave left for the current year.

Regarding next year, up to 8 unused leave days can be carried over to the next year. In your case, since you have 13 days left this year, you would be able to carry over:

13 (remaining leave) - 8 (carryover limit) = 5

So, you would start the new year with a balance of 5 carry-over leave days.

Code where we handle the promt stuffing:-
@GetMapping("/prompt-stuffing")
    public String promptStuffing(@RequestParam("message") String message) {
        return chatClient
                .prompt()
                .system(systemPromptTemplate)//this will override the system promt we set during bean initialization.
                .user(message)
                .call().content();
    }
=>Prompt stuffing is ineffective for large amounts of data due to several major drawbacks, including cost, latency, token limits, and performance degradation. For this reason, 
the industry-standard approach for handling large datasets is Retrieval-Augmented Generation (RAG)

----------------------------------------------------Advisiors(work same like filters in spring-----------------
What Are Advisors?
Advisors are components that act like interceptors, modifying or enhancing the flow of AI interactions, 
including requests and responses. They allow developers to encapsulate recurring patterns and cross-cutting concerns 
into reusable, portable units, much like Aspect-Oriented Programming (AOP) in traditional Spring applications. 
In Spring Al, advisors are like interceptors or middleware for your prompt flow.
They allow you to:
->Pre-process or post-process prompt data
->Add custom logging or auditing
->Inject additional behavior without modifying core logic
->Chain multiple behaviors cleanly
Where Advisors Fit
=>User ChatClient t [Advisors] LLM → Response → [Advisors] User
Best Practices
Keep advisors stateless or request-scoped
Chain multiple advisors if needed
Avoid altering the meaning of prompts unless intentional
Use advisors for cross-cutting concerns, not core logic
=======================================================When we config default advisor for logger, we cansee this =========
Default logging during chatClient bean creation, it will response the json intracted with llm, similerly we
can use the set of advisior which will be used to validate sensitive words ect.

public ChatController(ChatClient.Builder chatBuilder) {
        this.chatClient = chatBuilder.defaultAdvisors(new SimpleLoggerAdvisor()).build();
    }

Resposne:-
 "result" : {
    "output" : {
      "messageType" : "ASSISTANT",
      "metadata" : {
        "messageType" : "ASSISTANT"
      },
      "toolCalls" : [ ],
      "media" : [ ],
      "text" : "I don't have a personal name, but I'm an AI designed to assist and communicate with humans. I'm a type of language model, specifically a large language model (LLM), trained by Meta AI.\n\nAs for my \"model\" or architecture, I'm based on the transformer model introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. My specific architecture is a variant of this model, with some modifications and improvements made by my developers to enhance my language understanding and generation capabilities.\n\nIn terms of specific numbers, I was trained using a combination of supervised and unsupervised learning methods on a massive dataset of text from various sources, including books, articles, and online content. My training data is constantly updated and expanded to keep me up-to-date with the latest developments in language and knowledge.\n\nSo, while I don't have a personal name, you can think of me as \"LLaMA\" (Large Language Model Assistant) - a friendly AI designed to help answer your questions and provide information on a wide range of topics!"
    },
    "metadata" : {
      "finishReason" : "stop",
      "contentFilters" : [ ],
      "empty" : true
    }
  },
  "metadata" : {
    "id" : "",
    "model" : "llama3",
    "rateLimit" : {
      "requestsLimit" : 0,
      "tokensLimit" : 0,..............


->We have two advisors class which we implement for our custom advisor that are CallAdvisior and StreamAdvisior
In Spring AI, CallAdvisor and StreamAdvisor are interfaces used to intercept and modify the behavior of AI model interactions.
The key difference between them lies in the type of response they are designed to handle: CallAdvisor for synchronous,
blocking responses and StreamAdvisor for asynchronous, streaming responses. 

=>we have implemented our advisior here:-
public class TokenUsageAuditAdvisor implements CallAdvisor {//we will use the overided method to put our logic here

    private static final Logger logger = LoggerFactory.getLogger(TokenUsageAuditAdvisor.class);

    @Override
    public ChatClientResponse adviseCall(ChatClientRequest chatClientRequest, CallAdvisorChain callAdvisorChain) {
        ChatClientResponse chatClientResponse = callAdvisorChain.nextCall(chatClientRequest);//this will do the llm call,
          //we can use the before method to perform action before llm call, this signature can be taken from existing 
          //code implementation from CallAdvisor class.
        ChatResponse chatResponse = chatClientResponse.chatResponse();
        if(chatResponse.getMetadata() != null) {
            Usage usage = chatResponse.getMetadata().getUsage();//we can get the no of token used.

            if(usage != null) {
                logger.info("Token usage details : {}",usage.toString());
            }
        }
        return chatClientResponse;
    }

================================================using advisior at method level======================
@GetMapping("/chat")
    public String chat(@RequestParam("message") String message) {
        return chatClient.prompt()
                .advisors(List.of(new SimpleLoggerAdvisor(),new TokenUsageAuditAdvisor()))//list of advisior
                .user(message)
                .call().content();

    }

--------------------------------------What is ChatOptions?-----------------------------------
ChatOptions is a configuration in Spring Al that allows you to customize how a language model behaves during chat/completion calls.
Think of it like a "tuning panel" for your Al model - you can set limits, adjust creativity, randomness, verbosity, control response
length, and more.

Option                                Meaning
model                 Which LLM model to use (e.g., gpt-4, gpt-3.5-turbo, etc.)
frequencyPenalty      Reduces repetition. Higher = less repetition
presencePenalty        Encourages mentioning new topics
temparature            Controls creativity. 0 = focused, 1 = random
topP                    Controls randomness (nucleus sampling)
stopSequences         Stop generating when specific phrases are found
maxTokens              Maximum number of tokens (words/chars) in the reply
topK                  controls how many top choices are considered


e.g:-it can be applied at bean cration and controller level too:- this can be controller by .properties files too
public ChatController(ChatClient.Builder chatBuilder) {
        ChatOptions option=ChatOptions.builder()
                .maxTokens(10) //max response token
                .temperature(.8) 
                .topK(10) /top 10 answer
                .frequencyPenalty(.8)
                .build();

        this.chatClient = chatBuilder.defaultAdvisors(new SimpleLoggerAdvisor()).defaultOptions(option).build();
    }

  `````````````````````````````````````````````````Spring Al ChatClient - Response Types Explained```````````````````````
When you invoke.Call() on a ChatClient, there are different ways to get the result, depending on what you want to do with it.
Method                                       What It Returns                                   Use Case
content()                       Just the response as a String                         Simple use case - display or print reply
chatResponse()                   A ChatResponse object                                 Get full details like token usage
chatClientResponse()               A ChatClientResponse object                           Useful in RAG - includes context & metadata
entity(...) methods                 Converts response to POJOs                       Getting Java Objects (Structured Output)   


To stream reponses, we can use.stream() instead of.call()
Good for real-time or chunked responses (like streaming output to Ul

->@GetMapping("/stream")
    public Flux<String> stream(@RequestParam("message") String message) {
        return chatClient.prompt().user(message).stream().content();
    }

=>when we use stream , we have to make the return type Flux, its like steaming the data as it reacived , but in other case
if we hit the chat endpoint,we were waiting to get whole response then it is being respond to the postman, but here we 
have tired from the browser and we can see the dta is being generated and coming to screesn sequenctialluy.
by hitting this enpdoint:-http://localhost:8080/api/stream?message=what%20is%20the%20hr%20policies

---------------------------------------------------------Structured response form llm-----------
here in this controller we are using entity in the response to structure our response into POJO, when llm is being called
the instruction will be sent by spring AI to format in the requiured structure, since, default format is string response of
llm, here we have method which sent the instructions:-
e.g spring ai metthod:-

 public String getFormat() {
        return "Respond with only a list of comma-separated values, without any leading or trailing text.\nExample format: 
foo, bar, baz\n";
    }

THis is what being sent by spring AI in case of entity:---
its clearly sending the entity structure which we are trying to accept.

Do not include any explanations, only provide a RFC8259 compliant JSON response following this format without deviation.
Do not include markdown code blocks in your response.
Remove the ```json markdown from the output.
Here is the JSON Schema instance your output must adhere to:
```{
  "$schema" : "https://json-schema.org/draft/2020-12/schema",
  "type" : "object",
  "properties" : {
    "cities" : {
      "type" : "array",
      "items" : {
        "type" : "string"
      }
    },
    "country" : {
      "type" : "string"
    }
  },
  "additionalProperties" : false
}```



```````````````````````````````````````````````````Chat Memory using Spring Al-----------------------
Chat memory in Spring AI is the mechanism for maintaining conversational context across multiple interactions with a stateless Large Language Model (LLM). 
By storing and retrieving past messages, it enables the AI to provide coherent and context-aware responses.
Key components and details include:
=>when we saying my name is axy, but in next query if we ask what is your name, it does not answer,

when we have this setup:-
public ChatMemoryController(ChatClient.Builder chatBuilder, ChatMemory chatMemory) {
        Advisor logger= new SimpleLoggerAdvisor();
        Advisor memory = MessageChatMemoryAdvisor.builder(chatMemory).build();
        this.chatClient = chatBuilder.defaultAdvisors(List.of(logger, memory)).build();

    }

=>ChatMemory is an interface that provides a core abstraction for managing conversational history. It serves as the primary component for enabling stateful,
multi-turn conversations with a large language model (LLM), which are otherwise stateless. The interface defines methods to add, retrieve, and
clear messages associated with a specific conversation ID. 
=>MessageChatMemoryAdvisor is a component that enables a large language model (LLM) to retain conversational context by including the history of messages in the prompt. 
It is a built-in "advisor" that modifies the chat request before it is sent to the LLM. 

How Chat Memory Works:-
ChatMemory: Defines what to store (e.g, last N messages)
Sample implementation class MessageWindowChatMemory
=>ChatMemoryRepository: Defines contract for
storing and retrieving chat messages.
=>Using InMemoryChatMemoryRepository we can store the chat
memory inside In-Memory (default)
=>Using JdbcChatMemoryRepository we can store the chat
memory in a DB like H2, MySLQ, Postgres etc.


How MessageChatMemoryAdvisor works:-----------
The process involves these steps: 
=>Intercepts the request: A chat request is initiated by the user.
=>Retrieves memory: The MessageChatMemoryAdvisor retrieves the conversation history from an underlying ChatMemory implementation. By default, Spring AI uses an InMemoryChatMemory, which stores messages in a ConcurrentHashMap.
=>Adds messages to prompt: The advisor takes the retrieved messages and includes them as a collection of message objects in the outgoing request to the AI model. This maintains the conversational structure.
=>Updates memory: After the AI model responds, the advisor updates the ChatMemory with the new user and AI messages. 

-------->Here when we are using default conversationId , it will override the messages of if we update new name , so,we update our code same liek chat gpt (wrapper of llm)
they use session id to remember the chat, so similerly we can set our conversationId with userId, we will override this, so, we will able to get the details based on
userId set from postman.
---------->

@GetMapping("/chat-memory")
    public String chatMemory(@RequestHeader("userId") String userId, @RequestParam("message") String message) {
        return chatClient.prompt().
         advisors(advisorSpec -> advisorSpec.param(CONVERSATION_ID,userId))
        .user(message).call().content();
    }
--Now chat memory will have details of our converstain based on customerId.
public interface ChatMemory {
    String DEFAULT_CONVERSATION_ID = "default"; ///ealrier this was being used
    String CONVERSATION_ID = "chat_memory_conversation_id";//now this will be used in case of custom id.


-------
Linkage of ChatMemory:-
Abstraction: ChatMemory is the top-level interface for managing conversation history.
Persistence: ChatMemoryRepository is the lower-level interface for storage logic.
Behavior: MessageWindowChatMemory wraps a ChatMemoryRepository to provide intelligent windowing behavior, making the chat memory token-efficient.
Implementation: JdbcChatMemoryRepository provides a concrete, persistent storage mechanism by implementing ChatMemoryRepository and using JDBC to talk to a database like H2.
Integration: All these components are wired together, often via dependency injection in a framework like Spring, to provide a complete, persistent chat history solution

chatMemeory:- impments TokenWindowChatMemory,MessageWindowChatMemory
ChatMemoryRepository-> implemnts JdbcChatMemoryRepository,InMemoryChatMemoryRepository




@GetMapping("/random/chat")
    public ResponseEntity<String> randomChat(@RequestHeader("username") String username,
                                             @RequestParam("message") String message) {
        SearchRequest searchRequest =
                SearchRequest.builder().query(message)
                          .topK(3) //we will get three top data which are very near to our message
                        .similarityThreshold(0.5) //it will include the matching till half
                          .build();//this will set the query for the vector db.
        List<Document> similarDocs =  vectorStore.similaritySearch(searchRequest);
        String similarContext = similarDocs.stream()
                .map(Document::getText)
                .collect(Collectors.joining(System.lineSeparator()));//it will return the context from vector db
        String answer = chatClient.prompt()
                .system(promptSystemSpec -> promptSystemSpec.text(promptTemplate)
                        .param("documents", similarContext))//we will try to put the context to the promt template where instructions are given.
                .advisors(a -> a.param(CONVERSATION_ID, username)) //it will save our intraction for chat history
                .user(message)
                .call().content();
        return ResponseEntity.ok(answer);
    }


promt template:-{} doucument will be replaced by context respnse from vector db and if no context coming from vectorstore, it will response i don't know.

You are a helpful assistant, answering questions about EazyBytes company policies to it's
employees.

Given the context in the DOCUMENTS section and no prior knowledge, answer the employee's
question. If the answer is not in the DOCUMENTS section, then reply with "I don't know".

DOCUMENTS:
----------
{documents}
----------
